{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f802687-416a-4b38-82d3-ab387dc16c38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate pseudopeople simulated datasets\n",
    "\n",
    "The very first step is generating pseudopeople data that will be used both directly in the case study, and to create the reference files.\n",
    "Since this is an intensive operation and currently can only be distributed with Modin, we do only this step in this notebook, then\n",
    "save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a821e3e-9ae1-466d-b24f-66d1b0703a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pseudopeople as psp\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "# Importing pandas for access, regardless of whether we are using it as the compute engine\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4546dc-f329-48b7-9988-5d25991939f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac996f3-1270-4211-a829-728e3b2ba6f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# DO NOT EDIT if this notebook is not called generate_pseudopeople_simulated_datasets.ipynb!\n",
    "# This notebook is designed to be run with papermill; this cell is tagged 'parameters'\n",
    "# When you run this, save it to another filename.\n",
    "data_to_use = 'small_sample'\n",
    "output_dir = 'output'\n",
    "compute_engine = 'pandas'\n",
    "# Only matter if distributing\n",
    "num_jobs = 100\n",
    "cpus_per_job = 5\n",
    "memory_per_job = \"30GB\"\n",
    "very_noisy = True\n",
    "pseudopeople_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e90be-2f7f-46db-ae17-a3dae23b7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = str(Path(output_dir) / data_to_use / \"pseudopeople_simulated_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff29f5-473d-4dab-9e7f-453f284ad8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e6be4e-a9b4-47c3-ae1a-f6e7d2c9dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "psp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f1b1c7-e425-4fdb-a0fc-389e422d94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_engine == 'pandas':\n",
    "    import pandas as pd\n",
    "elif compute_engine.startswith('modin'):\n",
    "    if compute_engine.startswith('modin_dask_'):\n",
    "        import modin.config as modin_cfg\n",
    "        modin_cfg.Engine.put(\"dask\") # Use dask instead of ray (which is the default)\n",
    "\n",
    "        import dask\n",
    "\n",
    "        if compute_engine == 'modin_dask_distributed':\n",
    "            from dask_jobqueue import SLURMCluster\n",
    "\n",
    "            cluster = SLURMCluster(\n",
    "                queue='long.q',\n",
    "                account=\"proj_simscience\",\n",
    "                # If you give dask workers more than one core, they will use it to\n",
    "                # run more tasks at once, which can use more memory than is available.\n",
    "                # To have more than one thread per worker but use them all for\n",
    "                # multi-threading code in one task\n",
    "                # at a time, you have to set cores=1, processes=1 and job_cpu > 1.\n",
    "                cores=1,\n",
    "                processes=1,\n",
    "                memory=memory_per_job,\n",
    "                walltime=\"10-00:00:00\",\n",
    "                # Dask distributed looks at OS-reported memory to decide whether a worker is running out.\n",
    "                # If the memory allocator is not returning the memory to the OS promptly (even when holding onto it\n",
    "                # is smart), it will lead Dask to make bad decisions.\n",
    "                # By default, pyarrow uses jemalloc, but I could not get that to release memory quickly.\n",
    "                # Even this doesn't seem to be completely working, but in combination with small-ish partitions\n",
    "                # it seems to do okay -- unmanaged memory does seem to shrink from time to time, which it wasn't\n",
    "                # previously doing.\n",
    "                job_script_prologue=\"export ARROW_DEFAULT_MEMORY_POOL=system\\nexport MALLOC_TRIM_THRESHOLD_=0\",\n",
    "                job_cpu=cpus_per_job,\n",
    "                # NOTE: This is, as Dask requests, a directory local to the compute node.\n",
    "                # But IHME's cluster doesn't support this very well -- it can be small-ish,\n",
    "                # full of stuff from other users, etc.\n",
    "                local_directory=f\"/tmp/{os.environ['USER']}_dask_generate_pseudopeople_simulated_datasets\",\n",
    "                # NOTE: Network file system -- probably slow and doing a lot of unnecessary I/O!\n",
    "                # local_directory=f\"/ihme/scratch/users/{os.environ['USER']}/dask_work_dir/dask_generate_simulated_data\",\n",
    "                # HACK: Avoid nodes with /tmp too full (as of 11/17/2023)\n",
    "                job_extra_directives=[\"-x long-slurm-sarchive-p00[53-59]\"],\n",
    "                log_directory=f\"/ihme/temp/slurmoutput/{os.environ['USER']}\",\n",
    "            )\n",
    "\n",
    "            cluster.scale(n=num_jobs)\n",
    "            # Supposedly, this will start new jobs if the existing\n",
    "            # ones fail for some reason.\n",
    "            # https://stackoverflow.com/a/61295019\n",
    "            cluster.adapt(minimum_jobs=num_jobs, maximum_jobs=num_jobs)\n",
    "\n",
    "            from distributed import Client\n",
    "            client = Client(cluster)\n",
    "        else:\n",
    "            from distributed import Client\n",
    "            cpus_available = int(os.environ['SLURM_CPUS_ON_NODE'])\n",
    "            client = Client(n_workers=int(cpus_available / 2), threads_per_worker=2)\n",
    "\n",
    "        # Why is this necessary?!\n",
    "        # For some reason, if I don't set NPartitions, it seems to default to 0?!\n",
    "        num_row_groups = 1 if data_to_use == 'small_sample' else 334\n",
    "        modin_cfg.NPartitions.put(min(num_jobs * 3, num_row_groups))\n",
    "        modin_cfg.MinPartitionSize.put(1_000) # ensure no column-axis partitions -- they'll need to be joined up right away anyway by our row-wise noising\n",
    "\n",
    "        display(client)\n",
    "    elif compute_engine == 'modin_ray':\n",
    "        # Haven't worked on distributing this across multiple nodes\n",
    "        import ray\n",
    "        ray.init(runtime_env={'env_vars': {'__MODIN_AUTOIMPORT_PANDAS__': '1'}}, num_cpus=int(os.environ['SLURM_CPUS_ON_NODE']))\n",
    "    else:\n",
    "        # Use serial Python backend (good for debugging errors)\n",
    "        import modin.config as modin_cfg\n",
    "        modin_cfg.IsDebug.put(True)\n",
    "\n",
    "    import modin.pandas as pd\n",
    "\n",
    "    # https://modin.readthedocs.io/en/stable/usage_guide/advanced_usage/progress_bar.html\n",
    "    from modin.config import ProgressBar\n",
    "    ProgressBar.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08bfa0-f644-4faa-b303-ef24d9a32b86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abfcc66-502c-45f1-a269-d0c60fa5f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data_to_use in ('small_sample', 'usa')\n",
    "pseudopeople_input_dir = None if data_to_use == 'small_sample' else '/mnt/team/simulation_science/priv/engineering/vivarium_census_prl_synth_pop/results/release_02_yellow/full_data/united_states_of_america/2023_07_28_08_33_09/final_results/2023_08_16_09_58_54/pseudopeople_input_data_usa/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05826f-98d7-4dac-adf9-8d9f80809811",
   "metadata": {},
   "outputs": [],
   "source": [
    "psp_kwargs = {\n",
    "    'source': pseudopeople_input_dir,\n",
    "    'seed': pseudopeople_seed,\n",
    "}\n",
    "if 'modin' in compute_engine:\n",
    "    psp_kwargs['engine'] = 'modin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd7b4d-645d-4dca-94ae-ee1780048aa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Noise configuration\n",
    "\n",
    "In order to give ourselves more of a challenge, we significantly increase the amount of noise\n",
    "from the pseudopeople defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb002e-8468-4885-a4b5-52fdb75ce7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_configuration = psp.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42207b2-5f2b-40c1-82d8-4d6ca36fe4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for changing the default configuration according to a pattern\n",
    "def column_noise_value(dataset, column, noise_type, default_value):\n",
    "    if very_noisy and dataset in ('decennial_census', 'taxes_w2_and_1099', 'social_security'):\n",
    "        if noise_type == \"make_typos\":\n",
    "            if column == \"middle_initial\":\n",
    "                # 5% of middle initials (which are all a single token anyway) are wrong.\n",
    "                return {\"cell_probability\": 0.05, \"token_probability\": 1}\n",
    "            elif column in (\"first_name\", \"last_name\", \"street_name\"):\n",
    "                # 10% of these text columns were entered carelessly, at a rate of 1 error\n",
    "                # per 10 characters.\n",
    "                # The pseudopeople default is 1% careless.\n",
    "                return {\"cell_probability\": 0.1, \"token_probability\": 0.1}\n",
    "        elif noise_type == \"write_wrong_digits\" and (dataset != \"social_security\" or column != \"ssn\"):\n",
    "            # 10% of number columns were written carelessly, at a rate of 1 error\n",
    "            # per 10 characters.\n",
    "            # The pseudopeople default is 1% careless.\n",
    "            # Note that this is applied on top of (the default lower levels of) typos,\n",
    "            # since typos also apply to numeric characters.\n",
    "            # We never introduce error on the SSN in the SSA dataset\n",
    "            return {\"cell_probability\": 0.1, \"token_probability\": 0.1}\n",
    "\n",
    "    return default_value\n",
    "\n",
    "\n",
    "def row_noise_value(dataset, noise_type, default_value):\n",
    "    return default_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9dd19-eac0-455f-9af3-7bc7bc3029e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_configuration = {\n",
    "    dataset: {\n",
    "        noise_category: (\n",
    "            ({\n",
    "                column: {\n",
    "                    noise_type: column_noise_value(dataset, column, noise_type, noise_type_config)\n",
    "                    for noise_type, noise_type_config in column_config.items()\n",
    "                }\n",
    "                for column, column_config in noise_category_config.items()\n",
    "            }\n",
    "            if noise_category == \"column_noise\" else\n",
    "            {\n",
    "                noise_type: row_noise_value(dataset, noise_type, noise_type_config)\n",
    "                for noise_type, noise_type_config in noise_category_config.items()\n",
    "            })\n",
    "        )\n",
    "        for noise_category, noise_category_config in dataset_config.items()\n",
    "    }\n",
    "    for dataset, dataset_config in default_configuration.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8056884-bb09-475b-9499-56ba79b96c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "psp_kwargs['config'] = custom_configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d3767-1895-4c4f-91d8-72d13a19b5ec",
   "metadata": {},
   "source": [
    "### Simulated SSA Numident\n",
    "\n",
    "Wagner and Layne, p.4:\n",
    "\n",
    "> The reference files are derived from the Social Security Administration\n",
    "    (SSA) Numerical Identification file (SSA Numident). The Numident contains all\n",
    "    transactions recorded against one Social Security Number (SSN)...\n",
    "\n",
    "Based on the [SSA Numident through 2007 which is publicly available from NARA](https://aad.archives.gov/aad/series-description.jsp?s=5057),\n",
    "we know there are three kinds of transactions: SSN applications, deaths, and claiming benefits.\n",
    "SSN holders may change their information (e.g. changing name or sex) by submitting another application,\n",
    "which generates an additional application transaction.\n",
    "(The policies about this are found [on the SSA website](https://secure.ssa.gov/poms.nsf/lnx/0110212200).)\n",
    "\n",
    "The paper [\"Likely Transgender Individuals in U.S. Federal Administrative Records and the 2010 Census\" by Benjamin Cerf Harris](https://www.census.gov/content/dam/Census/library/working-papers/2015/adrm/carra-wp-2015-03.pdf)\n",
    "includes some helpful statistics (Table 2).\n",
    "The average person in the SSA Numident has 2.2 transactions (called \"claims\" in that paper, but with the same definition\n",
    "as our term \"transaction\": \"Any time an SSN is created or information associated with an existing SSN is changed, that event is registered\n",
    "as a claim.\").\n",
    "\n",
    "pseudopeople does not currently include correction, name change, or benefits claim transactions.\n",
    "It only includes SSN creation and death of the SSN holder.\n",
    "\n",
    "I've figured that there would be some delay in getting the Numident -- so by Census processing time\n",
    "for the 2030 Census, only the SSA transactions by the end of 2029 would be available.\n",
    "Note that with pseudopeople's current design it is only possible to set this cutoff at the end of a calendar year.\n",
    "The NORC report says that \"the Census NUMIDENT is recreated each year, to reflect\n",
    "Social Security transaction records through **March** of each year\" (p. 105),\n",
    "though it isn't clear when in the year the Census Numident is actually re-created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da0de7b-2e91-4d23-96bd-d331a820d7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "simulated_ssa_numident = psp.generate_social_security(\n",
    "    year=2029,\n",
    "    **psp_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a7f79-8054-4664-b916-f023585bf398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "simulated_ssa_numident.to_parquet(str(Path(output_dir) / \"simulated_ssa_numident.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe8e46b-f936-4704-b120-9bbcbce6c3e9",
   "metadata": {},
   "source": [
    "### Simulated 1040 tax filings\n",
    "\n",
    "We assume that the last 5 years of taxes would be available and used in the construction of the reference files -- see section about reference files below.\n",
    "\n",
    "Note that these are retrieved by *tax* year, so the 2029 taxes would be available in early 2030\n",
    "(around when our hypothetical case study is taking place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157a22b-45f3-4fc1-803e-4670a554b23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tax_years = list(range(2025, 2030))\n",
    "tax_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b9a92b7-bdaf-454d-8ad4-97db95d6b461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.35 s, sys: 607 ms, total: 5.96 s\n",
      "Wall time: 4.49 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for year in tax_years:\n",
    "    print(year)\n",
    "    psp.generate_taxes_1040(\n",
    "        year=year,\n",
    "        **psp_kwargs,\n",
    "    ).to_parquet(str(Path(output_dir) / f\"simulated_taxes_1040_{year}.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bad94f-c803-41e0-bf17-9ecfdb56cdbd",
   "metadata": {},
   "source": [
    "### Simulated W2/1099 tax filings\n",
    "\n",
    "We assume that the last 5 years of taxes would be available and used in the construction of the reference files.\n",
    "\n",
    "Note that these are retrieved by *tax* year, so the 2029 taxes would be available in early 2030\n",
    "(around when our hypothetical case study is taking place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c072c69-f7c7-4bf1-b99e-48c3257063ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.72 s, sys: 382 ms, total: 4.1 s\n",
      "Wall time: 3.16 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for year in tax_years:\n",
    "    print(year)\n",
    "    psp.generate_taxes_w2_and_1099(\n",
    "        year=year,\n",
    "        **psp_kwargs,\n",
    "    ).to_parquet(str(Path(output_dir) / f\"simulated_taxes_w2_and_1099_{year}.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6967cb-dfa6-4b43-98b9-e590d266f55f",
   "metadata": {},
   "source": [
    "### Simulated 2030 Census Unedited File (CUF)\n",
    "\n",
    "For now, we gloss over the data schema for addresses.\n",
    "We don't know how addresses would be formatted in the CUF (and it's hard to guess, because\n",
    "address is not part of the Census form), but it likely would have some of these fields\n",
    "(street number, street name, etc) combined.\n",
    "\n",
    "While PVS input files do not in general have names split into first, middle, and last,\n",
    "I am guessing the CUF **would** have first name, middle initial, last name (which is how pseudopeople\n",
    "generates it), because that [matches the Census questionnaire](https://www2.census.gov/programs-surveys/decennial/2020/technical-documentation/questionnaires-and-instructions/questionnaires/2020-informational-questionnaire-english_DI-Q1.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "670c2201-c8a3-43d8-8f3a-f98bf576666a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 495 ms, sys: 47.5 ms, total: 543 ms\n",
      "Wall time: 481 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "simulated_census_2030 = psp.generate_decennial_census(\n",
    "    year=2030,\n",
    "    **psp_kwargs,\n",
    ")\n",
    "simulated_census_2030.to_parquet(str(Path(output_dir) / f\"simulated_census_2030.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43c6bfe1-d95c-4774-8a1b-e477f258fca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 20 11:36:20 AM PST 2023\n"
     ]
    }
   ],
   "source": [
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19c70b-36c9-486f-9e65-6a32ca876ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate_simulated_data_3",
   "language": "python",
   "name": "generate_simulated_data_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

### Setup ###

import shutil, os

configfile: "snakemake_config_defaults.yaml"

if os.path.isfile("snakemake_config_overrides.yaml"):
    configfile: "snakemake_config_overrides.yaml"

# Need the full conda path for singularity (this path must be bound into container!)
import shutil
conda_path = shutil.which('conda')

# The first rule in the Snakefile is what will be generated by default.
# We have a dummy rule that depends on the accuracy notebook to ensure the
# entire workflow is run.
rule all:
    input:
        f'ground_truth_accuracy_{config["data_to_use"]}{config["custom_run_suffix"]}.ipynb'

# Snakemake requires a directory() wrapper around outputs when they are more than
# a single file.
# In our case, this depends on what compute_engine we are using -- pandas outputs
# parquet files while the distributed engines output parquet directories.
def get_directory_wrapper_if_necessary(papermill_params):
    if papermill_params["compute_engine"] == 'pandas':
        return lambda x: x
    else:
        return directory

# We use papermill to run the notebooks, instead of the built-in Snakemake integration,
# because it does not generate incremental output, nor output notebooks when there is
# an error. See https://github.com/snakemake/snakemake/pull/2857
def dict_to_papermill(d):
    return ' '.join([f'-p {k} {v}' for k, v in d.items()])

### Generate pseudopeople-simulated datasets ###

generate_pseudopeople_simulated_datasets_papermill_params = {
    'data_to_use': config["data_to_use"],
    'output_dir': f'{config["root_output_dir"]}/generate_simulated_data/',
    'very_noisy': config["data_to_use"] == 'small_sample',
    **config['papermill_params'][config["data_to_use"]]['generate_pseudopeople_simulated_datasets']
}

output_wrapper = get_directory_wrapper_if_necessary(generate_pseudopeople_simulated_datasets_papermill_params)

# Programmatically generate the list of all datasets this will output,
# since there are many
pseudopeople_simulated_datasets = {
    'census': [2030],
    'ssa_numident': None,
    'taxes_1040': range(2025, 2030),
    'taxes_w2_and_1099': range(2025, 2030),
}

pseudopeople_simulated_datasets_paths = []

for dataset_name, years in pseudopeople_simulated_datasets.items():
    if years is None:
        pseudopeople_simulated_datasets_paths.append(f'simulated_{dataset_name}.parquet')
    else:
        pseudopeople_simulated_datasets_paths += [
            f'simulated_{dataset_name}_{year}.parquet'
            for year in years
        ]

pseudopeople_simulated_datasets_paths = [
    f'{config["root_output_dir"]}/generate_simulated_data/{config["data_to_use"]}/pseudopeople_simulated_datasets/{p}'
    for p in pseudopeople_simulated_datasets_paths
]

rule generate_pseudopeople_simulated_datasets:
    input:
        "generate_simulated_data/generate_pseudopeople_simulated_datasets.ipynb"
    log: f'generate_simulated_data/generate_pseudopeople_simulated_datasets_{config["data_to_use"]}{config["custom_run_suffix"]}.ipynb'
    output:
        [output_wrapper(p) for p in pseudopeople_simulated_datasets_paths]
    conda: config["conda_environment_name"]
    shell:
        f"papermill {{input}} {{log}} {dict_to_papermill(generate_pseudopeople_simulated_datasets_papermill_params)} -k python3"

### Generate case study files ###

# "Case study files" here means the simulated CUF and reference files.
# These are processed versions of the pseudopeople outputs generated in
# the previous step.

generate_case_study_files_papermill_params = {
    'data_to_use': config["data_to_use"],
    'output_dir': f'{config["root_output_dir"]}/generate_simulated_data/',
    **config['papermill_params'][config["data_to_use"]]['generate_case_study_files']
}

output_wrapper = get_directory_wrapper_if_necessary(generate_case_study_files_papermill_params)

case_study_files = [
    'census_numident', 'alternate_dob_numident', 'alternate_name_numident',
    'geobase_reference_file', 'name_dob_reference_file',
    'census_2030',
]

case_study_files_paths = []

for file in case_study_files:
    case_study_files_paths += [
        f'simulated_{file}.parquet',
        f'simulated_{file}_ground_truth.parquet',
    ]

# This one doesn't follow the _ground_truth pattern.
case_study_files_paths += ['simulated_pik_simulant_pairs.parquet']

case_study_files_paths = [f'{config["root_output_dir"]}/generate_simulated_data/{config["data_to_use"]}/{p}' for p in case_study_files_paths]

rule generate_case_study_files:
    input:
        ["generate_simulated_data/generate_simulated_data.ipynb"] +
        pseudopeople_simulated_datasets_paths
    log: f'generate_simulated_data/generate_simulated_data_{config["data_to_use"]}{config["custom_run_suffix"]}.ipynb'
    output:
        [output_wrapper(p) for p in case_study_files_paths]
    conda: config["conda_environment_name"]
    shell:
        f"papermill {{input[0]}} {{log}} {dict_to_papermill(generate_case_study_files_papermill_params)} -k python3"

### Link datasets ###

link_datasets_papermill_params = {
    'data_to_use': config["data_to_use"],
    'input_dir': f'{config["root_output_dir"]}/generate_simulated_data/',
    'output_dir': f'{config["root_output_dir"]}/results/',
    **config['papermill_params'][config["data_to_use"]]['link_datasets']
}

output_wrapper = get_directory_wrapper_if_necessary(link_datasets_papermill_params)

linkage_outputs = [
    f'{config["root_output_dir"]}/results/{config["data_to_use"]}/census_2030_piked.parquet',
    f'{config["root_output_dir"]}/results/{config["data_to_use"]}/confirmed_piks.parquet',
]

rule link_datasets:
    container: "./spark.sif"
    benchmark:
        f'benchmarks/benchmark-{config["data_to_use"]}.txt'
    input:
        ["person_linkage_case_study.ipynb"] +
        [p for p in case_study_files_paths if 'ground_truth' not in p and 'pik_simulant_pairs' not in p]
    log: f'person_linkage_case_study_{config["data_to_use"]}{config["custom_run_suffix"]}.ipynb'
    output:
        [output_wrapper(p) for p in linkage_outputs]
    shell:
        f'{conda_path} run --no-capture-output -n {config["conda_environment_name"]} papermill {{input[0]}} {{log}} {dict_to_papermill(link_datasets_papermill_params)} -k python3'

### Calculate ground-truth accuracy ###

calculate_ground_truth_accuracy_papermill_params = {
    'data_to_use': config["data_to_use"],
    'simulated_data_output_dir': f'{config["root_output_dir"]}/generate_simulated_data/',
    'case_study_output_dir': f'{config["root_output_dir"]}/results/',
    **config['papermill_params'][config["data_to_use"]]['calculate_ground_truth_accuracy']
}

rule calculate_ground_truth_accuracy:
    input:
        ["ground_truth_accuracy.ipynb"] + case_study_files_paths + linkage_outputs
    log: f'ground_truth_accuracy_{config["data_to_use"]}{config["custom_run_suffix"]}.ipynb'
    conda: config["conda_environment_name"]
    shell:
        f"papermill {{input[0]}} {{log}} {dict_to_papermill(calculate_ground_truth_accuracy_papermill_params)} -k python3"


# For debugging, it may be helpful to run the notebooks through Snakemake, but in the browser.
# This can be achieved by replacing any shell command(s) with:
# f"""
# export HOME='' # Snakemake rules run with a non-default home directory, so you will want to set this to your home
# papermill --prepare-only {{input[0]}} {{log}} {dict_to_papermill(papermill_params)} -k python3
# jupyter lab --browser ':' --no-browser --ServerApp.quit_button=True --ip 0.0.0.0 --port 28508 {{log}}
# """
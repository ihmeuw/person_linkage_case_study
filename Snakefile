### Setup ###

import shutil, os, yaml

from layered_config_tree import LayeredConfigTree

snakemake_config = config

with open('config_defaults.yaml') as stream:
    config_defaults = yaml.safe_load(stream)

with open('config_overrides.yaml') as stream:
    config_overrides = yaml.safe_load(stream)

LAYERS = [
    ('defaults', config_defaults),
    ('user_file_overrides', config_overrides),
    ('snakemake_overrides', snakemake_config)
]

# https://stackoverflow.com/a/20666342
def merge_ignoring_all(source, destination):
    """
    run me with nosetests --with-doctest file.py

    >>> a = { 'all': 'foo', 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } }
    >>> b = { 'first' : { 'all_rows' : { 'all': 'bar', 'fail' : 'cat', 'number' : '5' } } }
    >>> merge_ignoring_all(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } }
    True
    """
    for key, value in source.items():
        # Zeb modification
        if key == "all":
            continue

        if isinstance(value, dict):
            # get node or create one
            node = destination.setdefault(key, {})
            merge_ignoring_all(value, node)
        else:
            destination[key] = value

    return destination

def merge_keys_by_level(list_of_keys_by_level_structures):
    all_levels = set()

    for list_of_keys_by_level_structure in list_of_keys_by_level_structures:
        all_levels |= list_of_keys_by_level_structure.keys()

    result = {}
    for level in all_levels:
        result[level] = set()
        for list_of_keys_by_level_structure in list_of_keys_by_level_structures:
            if level in list_of_keys_by_level_structure:
                result[level] |= list_of_keys_by_level_structure[level]
    
    return result

def all_possible_keys_by_level(config):
    top_level_keys = set()
    nested_keys = {}
    for k in config:
        top_level_keys |= {k}
        if isinstance(config[k], dict):
            nested_keys = merge_keys_by_level([nested_keys, all_possible_keys_by_level(config[k])])

    return {
        0: top_level_keys,
        **{k + 1: v for k, v in nested_keys.items()}
    }

def expand_all(config, level_keys=None):
    if "all" not in config:
        return config

    if level_keys is None:
        level_keys = all_possible_keys_by_level(config)

    all_config = expand_all(config["all"], {k - 1: v for k, v in level_keys.items() if k > 0})
    for k in level_keys[0]:
        if k == "all":
            continue

        if k in config and isinstance(config[k], dict):
            merge_ignoring_all(all_config, config[k])
        elif k not in config:
            config[k] = all_config

    del config["all"]
    return config

config = LayeredConfigTree(layers=[layer_name for layer_name, _ in LAYERS])

keys_by_level = {}

for layer, layer_contents in LAYERS:
    if 'papermill_params' in layer_contents:
        keys_by_level = merge_keys_by_level([keys_by_level, all_possible_keys_by_level(layer_contents['papermill_params'])])

for layer, layer_contents in LAYERS:
    if 'papermill_params' in layer_contents:
        layer_contents['papermill_params'] = expand_all(layer_contents['papermill_params'], keys_by_level)
    
    config.update(layer_contents, layer=layer)

# Need the full conda path for singularity (this path must be bound into container!)
import shutil
conda_path = shutil.which('conda')

# The first rule in the Snakefile is what will be generated by default.
# We have a dummy rule that depends on the accuracy notebook to ensure the
# entire workflow is run.
rule all:
    input:
        f'ground_truth_accuracy_{config["data_to_use"]}{config["custom_run_suffix"]}.ipynb'

# Snakemake requires a directory() wrapper around outputs when they are more than
# a single file.
# In our case, this depends on what compute_engine we are using -- pandas outputs
# parquet files while the distributed engines output parquet directories.
def get_directory_wrapper_if_necessary(papermill_params):
    if papermill_params["compute_engine"] == 'pandas':
        return lambda x: x
    else:
        return directory

# We use papermill to run the notebooks, instead of the built-in Snakemake integration,
# because it does not generate incremental output, nor output notebooks when there is
# an error. See https://github.com/snakemake/snakemake/pull/2857
def dict_to_papermill(d):
    return ' '.join([f'-p {k} {v}' for k, v in d.items()])

### Generate pseudopeople-simulated datasets ###

generate_pseudopeople_simulated_datasets_papermill_params = {
    'data_to_use': config["data_to_use"],
    'output_dir': f'{config["root_output_dir"]}/generate_simulated_data/',
    'very_noisy': config["data_to_use"] == 'small_sample',
    **dict(config["papermill_params"][config["data_to_use"]]["generate_pseudopeople_simulated_datasets"])
}

output_wrapper = get_directory_wrapper_if_necessary(generate_pseudopeople_simulated_datasets_papermill_params)

# Programmatically generate the list of all datasets this will output,
# since there are many
pseudopeople_simulated_datasets = {
    'census': [2030],
    'ssa_numident': None,
    'taxes_1040': range(2025, 2030),
    'taxes_w2_and_1099': range(2025, 2030),
}

pseudopeople_simulated_datasets_paths = []

for dataset_name, years in pseudopeople_simulated_datasets.items():
    if years is None:
        pseudopeople_simulated_datasets_paths.append(f'simulated_{dataset_name}.parquet')
    else:
        pseudopeople_simulated_datasets_paths += [
            f'simulated_{dataset_name}_{year}.parquet'
            for year in years
        ]

pseudopeople_simulated_datasets_paths = [
    f'{config["root_output_dir"]}/generate_simulated_data/{config["data_to_use"]}/pseudopeople_simulated_datasets/{p}'
    for p in pseudopeople_simulated_datasets_paths
]

rule generate_pseudopeople_simulated_datasets:
    input:
        "generate_simulated_data/generate_pseudopeople_simulated_datasets.ipynb"
    log: f'generate_simulated_data/generate_pseudopeople_simulated_datasets_{config["data_to_use"]}{config["custom_run_suffix"]}.ipynb'
    output:
        [output_wrapper(p) for p in pseudopeople_simulated_datasets_paths]
    conda: config["conda_environment_name"]
    shell:
        f"papermill {{input}} {{log}} {dict_to_papermill(generate_pseudopeople_simulated_datasets_papermill_params)} -k python3"

### Generate case study files ###

# "Case study files" here means the simulated CUF and reference files.
# These are processed versions of the pseudopeople outputs generated in
# the previous step.

generate_case_study_files_papermill_params = {
    'data_to_use': config["data_to_use"],
    'output_dir': f'{config["root_output_dir"]}/generate_simulated_data/',
    **dict(config["papermill_params"][config["data_to_use"]]["generate_case_study_files"]),
}

output_wrapper = get_directory_wrapper_if_necessary(generate_case_study_files_papermill_params)

case_study_files = [
    'census_numident', 'alternate_dob_numident', 'alternate_name_numident',
    'geobase_reference_file', 'name_dob_reference_file',
    'census_2030',
]

case_study_files_paths = []

for file in case_study_files:
    case_study_files_paths += [
        f'simulated_{file}.parquet',
        f'simulated_{file}_ground_truth.parquet',
    ]

# This one doesn't follow the _ground_truth pattern.
case_study_files_paths += ['simulated_pik_simulant_pairs.parquet']

case_study_files_paths = [f'{config["root_output_dir"]}/generate_simulated_data/{config["data_to_use"]}/{p}' for p in case_study_files_paths]

rule generate_case_study_files:
    input:
        ["generate_simulated_data/generate_simulated_data.ipynb"] +
        pseudopeople_simulated_datasets_paths
    log: f'generate_simulated_data/generate_simulated_data_{config["data_to_use"]}{config["custom_run_suffix"]}.ipynb'
    output:
        [output_wrapper(p) for p in case_study_files_paths]
    conda: config["conda_environment_name"]
    shell:
        f"papermill {{input[0]}} {{log}} {dict_to_papermill(generate_case_study_files_papermill_params)} -k python3"

### Link datasets ###

link_datasets_papermill_params = {
    'data_to_use': config["data_to_use"],
    'input_dir': f'{config["root_output_dir"]}/generate_simulated_data/',
    'output_dir': f'{config["root_output_dir"]}/results/',
    **dict(config["papermill_params"][config["data_to_use"]]["link_datasets"]),
}

output_wrapper = get_directory_wrapper_if_necessary(link_datasets_papermill_params)

linkage_outputs = [
    f'{config["root_output_dir"]}/results/{config["data_to_use"]}/census_2030_piked.parquet',
    f'{config["root_output_dir"]}/results/{config["data_to_use"]}/confirmed_piks.parquet',
]

rule link_datasets:
    container: "./spark.sif"
    benchmark:
        f'benchmarks/benchmark-{config["data_to_use"]}.txt'
    input:
        ["person_linkage_case_study.ipynb"] +
        [p for p in case_study_files_paths if 'ground_truth' not in p and 'pik_simulant_pairs' not in p]
    log: f'person_linkage_case_study_{config["data_to_use"]}{config["custom_run_suffix"]}.ipynb'
    output:
        [output_wrapper(p) for p in linkage_outputs]
    shell:
        f'{conda_path} run --no-capture-output -n {config["conda_environment_name"]} papermill {{input[0]}} {{log}} {dict_to_papermill(link_datasets_papermill_params)} -k python3'

### Calculate ground-truth accuracy ###

calculate_ground_truth_accuracy_papermill_params = {
    'data_to_use': config["data_to_use"],
    'simulated_data_output_dir': f'{config["root_output_dir"]}/generate_simulated_data/',
    'case_study_output_dir': f'{config["root_output_dir"]}/results/',
    **dict(config["papermill_params"][config["data_to_use"]]["calculate_ground_truth_accuracy"]),
}

rule calculate_ground_truth_accuracy:
    input:
        ["ground_truth_accuracy.ipynb"] + case_study_files_paths + linkage_outputs
    log: f'ground_truth_accuracy_{config["data_to_use"]}{config["custom_run_suffix"]}.ipynb'
    conda: config["conda_environment_name"]
    shell:
        f"papermill {{input[0]}} {{log}} {dict_to_papermill(calculate_ground_truth_accuracy_papermill_params)} -k python3"


# For debugging, it may be helpful to run the notebooks through Snakemake, but in the browser.
# This can be achieved by replacing any shell command(s) with:
# f"""
# export HOME='' # Snakemake rules run with a non-default home directory, so you will want to set this to your home
# papermill --prepare-only {{input[0]}} {{log}} {dict_to_papermill(papermill_params)} -k python3
# jupyter lab --browser ':' --no-browser --ServerApp.quit_button=True --ip 0.0.0.0 --port 28508 {{log}}
# """
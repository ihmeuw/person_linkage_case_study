{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9e49e5-db4e-4665-9892-0a679dcc22c5",
   "metadata": {},
   "source": [
    "# **Person linkage case study**\n",
    "\n",
    "In this case study, we imagine running a person linkage to add unique identifiers to a simulated 2030 Census Unedited File (CUF).\n",
    "We emulate the methods of the Census Bureau's Person Identification Validation System (PVS) using publicly available descriptions.\n",
    "The PVS aims to give people in the input file (the CUF, in this case) unique identifiers that can be used to link them to other administrative records.\n",
    "\n",
    "We approximate the (highly confidential) input data to PVS by using simulated data from our pseudopeople package.\n",
    "See the `generate_simulated_data` notebook for details about this.\n",
    "\n",
    "## Sources\n",
    "\n",
    "These papers describe PVS as their main subject:\n",
    "\n",
    "* Wagner and Layne. The Person Identification Validation System (PVS): Applying the Center for Administrative Records Research and Applications’ (CARRA) Record Linkage Software. 2014. https://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/carra-wp-2014-01.pdf ([archived](https://web.archive.org/web/20230216043235/https://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/carra-wp-2014-01.pdf))\n",
    "* Layne, Wagner, and Rothhaas. Estimating Record Linkage False Match Rate for the Person Identification Validation System. 2014. https://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/carra-wp-2014-02.pdf ([archived](https://web.archive.org/web/20220121051156/https://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/carra-wp-2014-02.pdf))\n",
    "* Alexander et al. Creating a Longitudinal Data Infrastructure at the Census Bureau. 2015. https://www.census.gov/content/dam/Census/library/working-papers/2015/adrm/2015-alexander.pdf ([archived](https://web.archive.org/web/20170723192315/http://www.census.gov/content/dam/Census/library/working-papers/2015/adrm/2015-alexander.pdf))\n",
    "* Massey and O'Hara. Person Matching in Historical Files using the Census Bureau’s Person Validation System. 2014. https://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/carra-wp-2014-11.pdf ([archived](https://web.archive.org/web/20221018074814/http://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/carra-wp-2014-11.pdf))\n",
    "* NORC. Assessment of the U.S. Census Bureau’s Person Identification Validation System. 2011. https://www.norc.org/content/dam/norc-org/pdfs/PVS%20Assessment%20Report%20FINAL%20JULY%202011.pdf ([archived](https://web.archive.org/web/20230705005935/https://www.norc.org/content/dam/norc-org/pdfs/PVS%20Assessment%20Report%20FINAL%20JULY%202011.pdf))\n",
    "\n",
    "These apply PVS to some linking task, and in doing so describe it in some detail:\n",
    "\n",
    "* Brown et al. Real-Time 2020 Administrative Record Census Simulation. 2023. https://www2.census.gov/programs-surveys/decennial/2020/program-management/evaluate-docs/EAE-2020-admin-records-experiment.pdf ([archived](https://web.archive.org/web/20230521191811/https://www2.census.gov/programs-surveys/decennial/2020/program-management/evaluate-docs/EAE-2020-admin-records-experiment.pdf))\n",
    "* Massey et al. Linking the 1940 U.S. Census with Modern Data. 2018. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6530596/ ([DOI](https://doi.org/10.1080%2F01615440.2018.1507772))\n",
    "* Luque and Wagner. Assessing Coverage and Quality of the 2007 Prototype Census Kidlink Database. 2015. https://www.census.gov/content/dam/Census/library/working-papers/2015/adrm/carra-wp-2015-07.pdf ([archived](https://web.archive.org/web/20220808205231/http://www.census.gov/content/dam/Census/library/working-papers/2015/adrm/carra-wp-2015-07.pdf))\n",
    "* Bond et al. The Nature of the Bias When Studying Only Linkable Person Records: Evidence from the American Community Survey. 2014. https://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/carra-wp-2014-08.pdf ([archived](https://web.archive.org/web/20220803083857/http://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/carra-wp-2014-08.pdf))\n",
    "\n",
    "Finally, this paper is about the address matching process (MAF Match) which acts as part of PVS:\n",
    "\n",
    "* Brummet. Comparison of Survey, Federal, and Commercial Address Data Quality. 2014. https://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/carra-wp-2014-06.pdf ([archived](https://web.archive.org/web/20220121213358/https://www.census.gov/content/dam/Census/library/working-papers/2014/adrm/carra-wp-2014-06.pdf))\n",
    "\n",
    "All of these papers are referenced throughout this notebook (and other notebooks) by the authors' names, along with additional information like page numbers.\n",
    "All page numbers represent those of the PDF files themselves, *not* page numbers printed in the documents.\n",
    "\n",
    "## PVS overview\n",
    "\n",
    "Brown et al., p. 28:\n",
    "\n",
    "> The PVS uses probabilistic record linkage (Fellegi and Sunter, 1969) to match data from an\n",
    "    incoming file (e.g., a survey or administrative record file) to reference files containing data on\n",
    "    SSN applications from the NUMIDENT enhanced with address data obtained from other federal\n",
    "    administrative records.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af1f28-7e30-4860-992e-15da22f9b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, copy, os, time, datetime\n",
    "import pandas, numpy\n",
    "import jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce58b3-0901-420f-b6c2-2dfd85e56f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10473b3a-f016-4afd-9e0e-cbaa968555e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae5f8b-f5b0-43db-907b-bdbc83a2d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport vivarium_research_prl\n",
    "from vivarium_research_prl import distributed_compute, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc4e2e-93cb-47f1-b4ab-fedac26382b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# DO NOT EDIT if this notebook is not called person_linkage_case_study.ipynb!\n",
    "# This notebook is designed to be run with papermill; this cell is tagged 'parameters'\n",
    "data_to_use = 'small_sample'\n",
    "input_dir = 'output/generate_simulated_data'\n",
    "output_dir = 'output'\n",
    "\n",
    "# The \"compute engine\" is what we use on the Python side\n",
    "# for our case-study-specific operations,\n",
    "# as opposed to the Splink engine\n",
    "compute_engine = 'pandas'\n",
    "# Only matter if using a distributed compute engine\n",
    "compute_engine_num_jobs = 3\n",
    "compute_engine_cpus_per_job = 2\n",
    "compute_engine_memory_per_job = \"1GB\"\n",
    "\n",
    "splink_engine = 'duckdb'\n",
    "# Only matter if using Spark\n",
    "spark_local = False\n",
    "# Only matter if using distributed (non-local) Spark\n",
    "spark_cpus_master = 2\n",
    "spark_memory_master = \"5GB\"\n",
    "spark_num_workers = 15\n",
    "spark_cpus_per_worker = 2\n",
    "spark_memory_per_worker = \"4GB\"\n",
    "spark_worker_memory_overhead_mb = 500\n",
    "\n",
    "clear_intermediate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd911b83-9295-46a4-957f-7de919ce451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters (WIP) for an RI run\n",
    "# data_to_use = \"usa\"\n",
    "# input_dir = \"/ihme/scratch/users/zmbc/person_linkage_case_study/generate_simulated_data\"\n",
    "# output_dir = \"/ihme/scratch/users/zmbc/person_linkage_case_study/person_linkage_case_study\"\n",
    "\n",
    "# compute_engine = 'dask'\n",
    "# compute_engine_num_jobs = 10\n",
    "# compute_engine_memory_per_job = \"5GB\"\n",
    "# compute_engine_cpus_per_job = 2\n",
    "\n",
    "# splink_engine = 'spark'\n",
    "# spark_num_workers = 50\n",
    "# spark_cpus_per_worker = 2\n",
    "# spark_memory_per_worker = \"30GB\"\n",
    "# spark_worker_memory_overhead_mb = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d42219-f60e-4547-8c98-2bf1b73af392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters (WIP) for a full USA run\n",
    "# data_to_use = \"usa\"\n",
    "# input_dir = \"/ihme/scratch/users/zmbc/person_linkage_case_study/generate_simulated_data\"\n",
    "# output_dir = \"/ihme/scratch/users/zmbc/person_linkage_case_study/person_linkage_case_study\"\n",
    "\n",
    "# compute_engine = 'dask'\n",
    "# compute_engine_num_jobs = 50\n",
    "# compute_engine_memory_per_job = \"120GB\"\n",
    "# compute_engine_cpus_per_job = 2\n",
    "\n",
    "# splink_engine = 'spark'\n",
    "# spark_cpus_master = 10\n",
    "# spark_memory_master = \"40GB\"\n",
    "# spark_num_workers = 50\n",
    "# spark_cpus_per_worker = 2\n",
    "# spark_memory_per_worker = \"120GB\"\n",
    "# spark_worker_memory_overhead_mb = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ba030-3ce6-489a-9659-296048287a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'{output_dir}/{data_to_use}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cdd3c-9b84-46fc-a024-adc6b197bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.environ[\"PATH\"] = f\"{Path('./slurm_within_singularity').resolve()}:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da04a59b-ed5b-43b9-a7b0-7e763e20be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ops, pd = distributed_compute.start_compute_engine(\n",
    "    compute_engine,\n",
    "    num_jobs=compute_engine_num_jobs,\n",
    "    cpus_per_job=compute_engine_cpus_per_job,\n",
    "    memory_per_job=compute_engine_memory_per_job,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab2072-f46d-49eb-a4d1-a89a30f1de36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdd565-6ae1-4bf9-b124-8e3d05d90143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "pwd = os.getcwd()\n",
    "slurm_output_dir = f\"{output_dir}/slurm_output\"\n",
    "if clear_intermediate:\n",
    "    utils.ensure_empty(slurm_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152189c1-e987-4000-b386-684d0938737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eccb943-b21f-46ed-a22f-2c200cd83a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if spark_memory_per_worker.endswith('G') or spark_memory_per_worker.endswith('GB'):\n",
    "    spark_memory_per_worker = int(re.search(r'(\\d+)GB?', spark_memory_per_worker).group(1)) * 1_000\n",
    "elif spark_memory_per_worker.endswith('M') or spark_memory_per_worker.endswith('MB'):\n",
    "    spark_memory_per_worker = int(re.search(r'(\\d+)MB?', spark_memory_per_worker).group(1))\n",
    "else:\n",
    "    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ca817-6be3-49ff-89fb-6fc6647f9500",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if splink_engine == 'spark':\n",
    "    if spark_local:\n",
    "        spark_master_url = \"local[2]\"\n",
    "    else:\n",
    "        log_path = f\"{slurm_output_dir}/spark-master-%j.out\"\n",
    "        array_job_log_path = f\"{slurm_output_dir}/spark-worker-%A_%a.out\"\n",
    "        print(\"Starting Spark master\")\n",
    "        spark_start_master_output = os.popen(\n",
    "            f\"sbatch --output {log_path} --error {log_path} \"\n",
    "            f\"--cpus-per-task {spark_cpus_master} --mem {spark_memory_master} \"\n",
    "            f\"-A proj_simscience -p long.q {pwd}/start_spark_master.sh\"\n",
    "        ).read()\n",
    "        spark_master_job_id = re.match('Submitted batch job (\\d+)', spark_start_master_output)[1]\n",
    "\n",
    "        # TODO: This cleanup is not actually working\n",
    "        import atexit\n",
    "        \n",
    "        def kill_spark_master():\n",
    "            os.popen(f\"scancel {spark_master_job_id}\").read()\n",
    "        \n",
    "        atexit.register(kill_spark_master)\n",
    "\n",
    "        while True:\n",
    "            logs = os.popen(f\"ssh $(hostname) -q -t cat {log_path.replace('%j', spark_master_job_id)} 2>&1\").read()\n",
    "            starting_lines = [l for l in logs.split('\\n') if 'Starting Spark master at' in l]\n",
    "            webui_starting_lines = [l for l in logs.split('\\n') if 'Bound MasterWebUI to' in l]\n",
    "            if len(starting_lines) > 0 and len(webui_starting_lines) > 0:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(5)\n",
    "\n",
    "        spark_master_url = re.match('.*Starting Spark master at (spark:.+)$', starting_lines[0])[1]\n",
    "        spark_master_webui_url = re.match('.*Bound MasterWebUI to .*, and started at (http:.+)$', webui_starting_lines[0])[1]\n",
    "        print(f'Got Spark master URL: {spark_master_url}')\n",
    "        print('\\n'.join([l for l in logs.split('\\n') if 'Bound MasterWebUI' in l]))\n",
    "\n",
    "        spark_worker_job_ids = []\n",
    "\n",
    "        def start_spark_workers(n_workers):\n",
    "            spark_start_workers_output = os.popen(\n",
    "                f\"sbatch --output {array_job_log_path} --error {array_job_log_path} \"\n",
    "                f\"--array=1-{n_workers} --cpus-per-task {spark_cpus_per_worker} --mem {spark_memory_per_worker} \"\n",
    "                f\"-A proj_simscience -p long.q {pwd}/start_spark_workers.sh {spark_master_url} {spark_worker_memory_overhead_mb}\"\n",
    "            ).read()\n",
    "            job_id = re.match('Submitted batch job (\\d+)', spark_start_workers_output)[1]\n",
    "            spark_worker_job_ids.append(job_id)\n",
    "\n",
    "            while True:\n",
    "                logs = os.popen(f\"ssh $(hostname) -q -t cat {array_job_log_path.replace('%A', job_id).replace('%a', '*')} 2>&1\").read()\n",
    "                starting_lines = [l for l in logs.split('\\n') if 'Starting Spark worker' in l]\n",
    "                if len(starting_lines) == n_workers:\n",
    "                    print('\\n'.join([l for l in logs.split('\\n') if 'Starting Spark worker' in l or 'Bound WorkerWebUI' in l]))\n",
    "                    break\n",
    "                else:\n",
    "                    time.sleep(5)\n",
    "\n",
    "            return job_id\n",
    "\n",
    "        print('Starting Spark workers')\n",
    "        spark_workers_job_id = start_spark_workers(spark_num_workers)\n",
    "\n",
    "        def kill_spark_workers():\n",
    "            for job_id in spark_worker_job_ids:\n",
    "                os.popen(f\"scancel {job_id}\").read()\n",
    "\n",
    "        atexit.register(kill_spark_workers)\n",
    "\n",
    "        import asyncio\n",
    "        import requests\n",
    "\n",
    "        def maintain_spark_worker_count():\n",
    "            while True:\n",
    "                alive_workers = requests.get(f'{spark_master_webui_url}/json/').json()['aliveworkers']\n",
    "                if alive_workers < spark_num_workers:\n",
    "                    num_to_start = spark_num_workers - alive_workers\n",
    "                    print(f\"Starting {num_to_start} more Spark worker(s)\")\n",
    "                    start_spark_workers(num_to_start)\n",
    "                time.sleep(20)\n",
    "\n",
    "        def stop():\n",
    "            task.cancel()\n",
    "\n",
    "        atexit.register(stop)\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        task = loop.run_in_executor(None, maintain_spark_worker_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e8c9a9-d691-4110-baed-683004701894",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "See code in `generate_simulated_data` directory for how we generated the files to link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1086b-639c-4421-95ef-dc4108b7fd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "census_2030 = df_ops.persist(df_ops.read_parquet(f'{input_dir}/{data_to_use}/simulated_census_2030.parquet'))\n",
    "geobase_reference_file = df_ops.persist(df_ops.read_parquet(f'{input_dir}/{data_to_use}/simulated_geobase_reference_file.parquet'))\n",
    "name_dob_reference_file = df_ops.persist(df_ops.read_parquet(f'{input_dir}/{data_to_use}/simulated_name_dob_reference_file.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6d16d-f4c9-4c1a-b969-baee5ac539ad",
   "metadata": {},
   "source": [
    "# Pre-process input file\n",
    "\n",
    "Wagner and Layne, p. 9:\n",
    "\n",
    "> The first step of the PVS process is to edit data fields to make them homogenous for\n",
    "comparisons between incoming and reference files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0111f6-d8e0-4758-a242-70027d67b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file before any processing; the final result will be this with the PIK column added\n",
    "census_2030_raw_input = census_2030.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cf59a-804c-4372-bcbf-740ec724b3ee",
   "metadata": {},
   "source": [
    "## Name parsing and standardizing\n",
    "\n",
    "Wagner and Layne, p. 9:\n",
    "\n",
    "> The first edits are parsing and standardizing - parsing separates fields into\n",
    "component parts, while standardizing guarantees key data elements are consistent (e.g.,\n",
    "STREET, STR are both converted to ST). Name and address fields are parsed and\n",
    "standardized as they are key linkage comparators. \n",
    "\n",
    "As noted in the data generation notebook, there is no parsing to be done here:\n",
    "because the Census questionnnaire asks for first name, middle initial, and last name as\n",
    "separate fields on the form, the name would be already parsed when running PVS on the CUF.\n",
    "\n",
    "The real PVS name parser generates fields like prefix (e.g. Mr.) and suffix (e.g. Jr.)\n",
    "but we never have these in names from pseudopeople.\n",
    "\n",
    "More details about the name parser and standardizer (Wagner and Layne, p. 10):\n",
    "\n",
    "> The PVS system incorporates a name\n",
    "standardizer (McGaughey, 1994), which is a C language subroutine called as a function\n",
    "within SAS. It performs name parsing and includes a nickname lookup table and outputs\n",
    "name variants (standardized variations of first and last names). For example, Bill\n",
    "becomes William, Chuck and Charlie becomes Charles, etc. The PVS keeps both the\n",
    "original name (Bill) and the converted name (William) for matching. PVS also has a fake\n",
    "name table to blank names such as “Queen of the House” or “Baby Girl.” The name data\n",
    "are parsed, checked for nicknames, and standardized.\n",
    "\n",
    "The key thing that wasn't clear to me from this description was what it meant to\n",
    "\"keep both original name and the converted name for matching.\"\n",
    "I found this in Massey et al.:\n",
    "\n",
    "> The Name Search Module also accounts for instances where census records contain a nickname.\n",
    "> For these records, the preprocessing step of the Name Search module outputs two records for\n",
    "> these observations, one record for the nickname and one record for the formal name.\n",
    "> For example, if the input record has the name “Bill Smith,” the formatting program will add\n",
    "> a formal name “William” to that record. This record will then output to both the B-S cut and to the W-S cut.\n",
    "\n",
    "From this, I gather that nicknames work similar to alternate names in the reference file:\n",
    "entire duplicate records are made with each name (nickname and formal).\n",
    "However, I am still confused by calling this \"the preprocessing step **of the Name Search module**.\"\n",
    "In Wagner and Layne it does not seem like this is module-specific, and I don't see any\n",
    "reason that would be desirable, so I have done this multi-record approach across all modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b449705-cd3f-4a14-9520-b4dfb1fa481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nickname processing\n",
    "# Have not yet found a nickname list in PVS docs,\n",
    "# so we do a minimal version for now -- could use\n",
    "# another list such as the one in pseudopeople\n",
    "# These examples all come directly from examples in the descriptions of PVS\n",
    "nickname_standardizations = {\n",
    "    \"Bill\": \"William\",\n",
    "    \"Chuck\": \"Charles\",\n",
    "    \"Charlie\": \"Charles\",\n",
    "    \"Cathy\": \"Catherine\",\n",
    "    \"Matt\": \"Matthew\",\n",
    "}\n",
    "has_nickname = df_ops.persist(census_2030.first_name.isin(nickname_standardizations.keys()))\n",
    "print(f'{df_ops.compute(has_nickname.sum()):,.0f} nicknames in the Census')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b119152-d5db-4d78-bab9-b15feae95f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra rows for the normalized names\n",
    "census_2030 = df_ops.concat([\n",
    "    census_2030,\n",
    "    census_2030[has_nickname].assign(first_name=lambda df: df.first_name.replace(nickname_standardizations))\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25cd47-aab9-42ee-98a5-3bdc3d66c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The above will introduce duplicates on record_id, so we redefine\n",
    "# record_id to be unique (without getting rid of the original, input file record ID)\n",
    "census_2030 = df_ops.add_unique_record_id(\n",
    "    census_2030.rename(columns={'record_id': 'record_id_raw_input_file'}),\n",
    "    \"census_2030_preprocessed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086540a-d9d0-4068-8ee8-539314aa91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list of fake names comes from the NORC report, p. 100-101\n",
    "# It is what was used in PVS as of 2011\n",
    "fake_names = pd.read_csv('fake-names.txt', header=None).rename(columns={0: 'fake_name'})\n",
    "fake_names = df_ops.persist(\n",
    "    fake_names\n",
    "        .assign(fake_name=fake_names.fake_name.str.strip().str.upper())\n",
    "        .dropna(subset='fake_name')\n",
    "        .pipe(df_ops.drop_duplicates)\n",
    "        .pipe(df_ops.ensure_large_string_capacity)\n",
    ")\n",
    "\n",
    "assert df_ops.compute((fake_names.fake_name == fake_names.fake_name.str.upper()).all())\n",
    "fake_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234ee61-5d2a-490e-97ad-eff72df732ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"first_name\", \"last_name\"]:\n",
    "    census_2030 = census_2030.assign(**{f'{col}_upper': census_2030[col].str.upper()}).merge(fake_names, left_on=f'{col}_upper', right_on='fake_name', how='left')\n",
    "    has_fake_name = df_ops.persist(census_2030.fake_name.notnull())\n",
    "    print(f'Found {df_ops.compute(has_fake_name.sum()):,.0f} fake names in {col}')\n",
    "    census_2030 = df_ops.concat([\n",
    "        census_2030[~has_fake_name].drop(columns=['fake_name', f'{col}_upper']),\n",
    "        # NOTE: For reasons I don't really understand, assigning pandas.NA reverts it to a normal string instead of a large_string\n",
    "        census_2030[has_fake_name].assign(**{col: pandas.NA}).drop(columns=['fake_name', f'{col}_upper']).pipe(df_ops.ensure_large_string_capacity),\n",
    "    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26659727-027b-4daf-8921-1fdb4b5e4d54",
   "metadata": {},
   "source": [
    "## Address parsing and standardizing\n",
    "\n",
    "Wagner and Layne, p. 11:\n",
    "\n",
    "> The PVS editing process also incorporates an address parser and standardizer,\n",
    "written in the C language and called as a function within SAS (U.S. Census Bureau\n",
    "Geography Division, 1995). It performs parsing of address strings into individual output\n",
    "fields (see Figure 2), and standardizes the spelling of key components of the address\n",
    "such as street type. The PVS also incorporates use of a commercial product to update\n",
    "zip codes, and correct misspellings of address elements.\n",
    "\n",
    "As noted in the data generation notebook, for now we haven't combined the address parts\n",
    "into a single string that would need to be parsed.\n",
    "We plan to add this in a future version of pseudopeople.\n",
    "\n",
    "For now, we do some simple standardization in each of the address parts (street number,\n",
    "street name, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435bc12-0002-44bc-91e3-26083d95c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_address_part(column):\n",
    "    return (\n",
    "        column\n",
    "            # Remove leading or trailing whitespace\n",
    "            .str.strip()\n",
    "            # Turn any strings of consecutive whitespace into a single space\n",
    "            .str.replace('\\s+', ' ', regex=True)\n",
    "            # Normalize case\n",
    "            .str.upper()\n",
    "            # Normalize the word street as described in the example quoted above\n",
    "            # In reality, there would be many rules like this\n",
    "            .str.replace('\\b(STREET|STR)\\b', 'ST', regex=True)\n",
    "            # Make sure missingness is represented consistently\n",
    "            .replace('', pandas.NA)\n",
    "    )\n",
    "\n",
    "address_cols = ['street_number', 'street_name', 'unit_number', 'city', 'state', 'zipcode']\n",
    "for address_col in address_cols:\n",
    "    census_2030 = census_2030.assign(**{address_col: census_2030[address_col].pipe(standardize_address_part)})\n",
    "\n",
    "census_2030 = df_ops.ensure_large_string_capacity(census_2030)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b55b52-c3ad-41da-9351-05ef56927dbf",
   "metadata": {},
   "source": [
    "## MAFMatch\n",
    "\n",
    "**General information about MAFMatch**\n",
    "\n",
    "Wagner and Layne, p. 11:\n",
    "\n",
    "> The PVS provides an additional address enhancement by matching records in the\n",
    "incoming file to Census Bureau’s Master Address File (MAF) in order to assign a unique\n",
    "address identifier, the MAF Identifier (MAFID), and other Census geographical codes\n",
    "(e.g., Census tract and block). The MAFID is used in the PVS for search purposes and as a\n",
    "linkage key for administrative files. Then, addresses are matched to the Census Bureau’s\n",
    "Topologically Integrated Geographic Encoding and Referencing Database (TIGER) to\n",
    "obtain Census geographical codes.\n",
    "\n",
    "My understanding is that this step is useful for two reasons:\n",
    "- Potentially adds an alternate version/way of formatting the same address\n",
    "  (the way it is in the MAF, instead of the way it is in the input file).\n",
    "  However, instead of using this alternate to match directly, it is boiled down\n",
    "  to a MAFID, so it is only useful when an input file address and a reference file\n",
    "  address (different to one another) both match to the same MAF address.\n",
    "  (Presumably, the whole MAFMatch process described here needs to run on the reference files?)\n",
    "- Adds Census geographies which could be used in blocking (but are they?)\n",
    "\n",
    "I don't understand the TIGER part of this.\n",
    "In particular, p. 12 describes a probabilistic/fuzzy match to TIGER, but I thought TIGER\n",
    "would contain exactly the same addresses as the MAF.\n",
    "I also don't understand what geocodes would be present in TIGER that wouldn't also be\n",
    "present in the MAF.\n",
    "Maybe these things are more out-of-sync with each other than I understood.\n",
    "\n",
    "**MAFMatch in this case study**\n",
    "\n",
    "In the decennial Census, the sampling frame is a subset of the MAF (Brown et al., p. 15, footnote 4).\n",
    "That is, in the CUF, the address field (which is not supplied by the respondent)\n",
    "would be the MAF address the questionnaire/NRFU was sent to.\n",
    "\n",
    "Therefore, I don't believe it makes sense to do MAFMatch for the CUF, because all the addresses\n",
    "are already the same.\n",
    "I haven't fully confirmed this, but p. 14 of Wagner and Layne says\n",
    "\n",
    "> The 2010 Census Unedited File (CUF), had 350 million records and processed through every PVS\n",
    "module, excluding MAF match and SSN verification\n",
    "\n",
    "which suggests that (like SSN verification) MAFMatch is not applicable to the CUF.\n",
    "\n",
    "Given this, we skip MAFMatch here.\n",
    "Also, if we wanted to add it, we would need to generate something like a MAF from pseudopeople."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ec2c6-a028-4f36-a041-63627fb60fd6",
   "metadata": {},
   "source": [
    "## Drop records with insufficient information\n",
    "\n",
    "In 2011 (NORC, p. 25):\n",
    "\n",
    "> The initial edit process, described in the Introduction: PVS Background section, removes from consideration\n",
    "incoming records that have no name data. Therefore, no record that is processed in PVS has blank first and last\n",
    "names.\n",
    "\n",
    "In 2014 it appears to be the same/similar, because in Table 2 of Wagner and Layne there is a row \"NO SEARCH: Blank Name\" (p. 18).\n",
    "\n",
    "In 2023 (Brown et al., p. 28):\n",
    "\n",
    "> Records containing sufficient PII to be linkable with some confidence, for example those\n",
    "containing name and age, are sent through the linkage process.<sup>15</sup>\n",
    "\n",
    "> <sup>15</sup> Records with names on the PVS invalid name list (e.g., “Mickey Mouse,” “householder,” or “son”) are excluded\n",
    "from PVS search.\n",
    "\n",
    "I'd prefer to use the 2023 information, but it is too vague:\n",
    "\"for example\" means this is just an approximation, and it doesn't specify\n",
    "parts of \"name.\"\n",
    "The footnote also seems to contradict earlier reports, which said fake names were simply\n",
    "blanked out, which seems preferable.\n",
    "Since the fake name step comes before this one, a fake first **and** last name would lead\n",
    "to exclusion here, which is perhaps what the footnote was intending?\n",
    "\n",
    "Here, we follow the blank-name approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c1379-c46b-4a0f-8011-596efae84778",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_2030 = census_2030[\n",
    "    census_2030.first_name.notnull() |\n",
    "    census_2030.last_name.notnull()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a91f7-e733-4a3d-bd56-7e23b0c8d138",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create derived variables for use in linkage\n",
    "\n",
    "Here we create variables to be used as matching variables and blocking keys,\n",
    "when those matching variables/blocking keys are defined in a way that is not already\n",
    "present in the data at this point.\n",
    "\n",
    "The variables needed here depend on the modules and passes described below -- see those\n",
    "sections for more citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb97b21-8ca1-4ad3-bae1-dbf7dc0bee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to compare mailing address with physical address\n",
    "geobase_reference_file = geobase_reference_file.rename(columns=lambda c: c.replace('mailing_address_', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c2257-a840-498a-9aa5-0788f9763634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PVS uses DOB as separate fields for day, month, and year\n",
    "def split_dob(df, date_format='%Y%m%d'):\n",
    "    # Have to be floats because we want to treat as numeric for assessing similarity\n",
    "    # Note that as of now, none of our pseudopeople noise types would change the punctuation (\"/\") in the date, but\n",
    "    # they can insert non-numeric characters here or otherwise create invalid dates, in which case we fail to parse the date\n",
    "    # and treat it as missing.\n",
    "    dob = pd.to_datetime(df.date_of_birth, format=date_format, errors='coerce')\n",
    "    df = df.assign(\n",
    "        month_of_birth=dob.dt.month,\n",
    "        year_of_birth=dob.dt.year,\n",
    "        day_of_birth=dob.dt.day,\n",
    "    )\n",
    "    df = df.assign(\n",
    "        month_of_birth=df.month_of_birth.fillna(pandas.NA),\n",
    "        year_of_birth=df.year_of_birth.fillna(pandas.NA),\n",
    "        day_of_birth=df.day_of_birth.fillna(pandas.NA),\n",
    "    )\n",
    "    return df.drop(columns=['date_of_birth'])\n",
    "\n",
    "census_2030 = split_dob(census_2030, date_format='%m/%d/%Y')\n",
    "geobase_reference_file = split_dob(geobase_reference_file)\n",
    "name_dob_reference_file = split_dob(name_dob_reference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55126ed5-94e3-4713-badd-9861f5c6ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't fully understand the purpose of blocking on the geokey,\n",
    "# as opposed to just blocking on its constituent columns.\n",
    "# Maybe it is a way of dealing with missingness in those constituent\n",
    "# columns (e.g. so an address with no unit number can still be blocked on geokey)?\n",
    "def add_geokey(df):\n",
    "    return (\n",
    "        df.assign(\n",
    "            geokey=distributed_compute.add_strings(\n",
    "                compute_engine,\n",
    "                [\n",
    "                    df.street_number, ' ',\n",
    "                    df.street_name, ' ',\n",
    "                    df.unit_number, ' ',\n",
    "                    df.city, ' ',\n",
    "                    df.state, ' ',\n",
    "                    df.zipcode,\n",
    "                ]\n",
    "                # Normalize the whitespace\n",
    "            ).str.replace('\\s+', ' ', regex=True).str.strip().replace('', pandas.NA)\n",
    "        )\n",
    "        .pipe(df_ops.ensure_large_string_capacity)\n",
    "    )\n",
    "\n",
    "geobase_reference_file = add_geokey(geobase_reference_file)\n",
    "census_2030 = df_ops.persist(add_geokey(census_2030))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51e08a-8e28-4c0f-9a62-d6207a560f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK: Remove address information from GQ, so it isn't used for blocking\n",
    "# nor for HHCompSearch.\n",
    "# Currently we have massive GQ \"households.\"\n",
    "# NOTE: This will miss people who are actually in GQ who have had their address noised,\n",
    "# but that doesn't create huge \"households\" anyway so it's not a big deal\n",
    "# NOTE: We can't just throw out any household where anyone has a GQ housing_type\n",
    "# since the housing_type column itself can be noised. We set a threshold (data size dependent)\n",
    "# of how many people in a household need to have a GQ housing_type.\n",
    "unlikely_due_to_noise_threshold = 250 if data_to_use == 'usa' else 2\n",
    "probable_gq_geokeys = df_ops.persist(\n",
    "    census_2030[census_2030.housing_type.notnull()].assign(reported_gq=lambda df: df.housing_type != 'Household')\n",
    "        .pipe(df_ops.groupby_agg_small_groups, by=\"geokey\", agg_func=lambda x: x.reported_gq.agg([\"sum\", \"mean\"]))\n",
    "        .pipe(lambda df: df[(df['sum'] >= unlikely_due_to_noise_threshold) & (df['mean'] >= 0.70)])\n",
    "        .reset_index()\n",
    "        .assign(is_gq_geokey=True)\n",
    ")\n",
    "\n",
    "if data_to_use in ('usa', 'ri'):\n",
    "    num_shards = 334\n",
    "    num_gq_types = 6\n",
    "    if data_to_use == 'usa':\n",
    "        # Should be exact\n",
    "        assert num_shards * num_gq_types == len(probable_gq_geokeys)\n",
    "    else:\n",
    "        # Should be close\n",
    "        print(f'{len(probable_gq_geokeys):,.0f} probable GQ geokeys')\n",
    "        assert (num_shards * num_gq_types * 0.75) <= len(probable_gq_geokeys) <= (num_shards * num_gq_types * 1.25)\n",
    "else:\n",
    "    print(f'{len(probable_gq_geokeys):,.0f} probable GQ geokeys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f92cd7-9a57-4ea1-a97a-c091ea335a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_2030 = census_2030.merge(probable_gq_geokeys[['geokey', 'is_gq_geokey']], on=\"geokey\", how=\"left\").assign(is_gq_geokey=lambda df: df.is_gq_geokey.fillna(False))\n",
    "\n",
    "census_2030 = df_ops.ensure_large_string_capacity(df_ops.concat([\n",
    "    census_2030[~census_2030.is_gq_geokey].assign(\n",
    "        geokey_for_blocking=lambda df: df.geokey,\n",
    "        street_number_for_blocking=lambda df: df.street_number,\n",
    "        street_name_for_blocking=lambda df: df.street_name,\n",
    "    ).drop(columns=[\"is_gq_geokey\"]),\n",
    "    census_2030[census_2030.is_gq_geokey].assign(\n",
    "        geokey_for_blocking=pandas.NA,\n",
    "        street_number_for_blocking=pandas.NA,\n",
    "        street_name_for_blocking=pandas.NA,\n",
    "    ).drop(columns=[\"is_gq_geokey\"]),\n",
    "], ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10958fd-adad-4bb2-b4c2-641f6ad242e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "geobase_reference_file = geobase_reference_file.merge(probable_gq_geokeys[['geokey', 'is_gq_geokey']], on=\"geokey\", how=\"left\").assign(is_gq_geokey=lambda df: df.is_gq_geokey.fillna(False))\n",
    "\n",
    "geobase_reference_file = df_ops.ensure_large_string_capacity(df_ops.concat([\n",
    "    geobase_reference_file[~geobase_reference_file.is_gq_geokey].assign(\n",
    "        geokey_for_blocking=lambda df: df.geokey,\n",
    "        street_number_for_blocking=lambda df: df.street_number,\n",
    "        street_name_for_blocking=lambda df: df.street_name,\n",
    "    ).drop(columns=[\"is_gq_geokey\"]),\n",
    "    geobase_reference_file[geobase_reference_file.is_gq_geokey].assign(\n",
    "        geokey_for_blocking=pandas.NA,\n",
    "        street_number_for_blocking=pandas.NA,\n",
    "        street_name_for_blocking=pandas.NA,\n",
    "    ).drop(columns=[\"is_gq_geokey\"]),\n",
    "], ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a686489-f265-4647-ad1d-7b6b93f8578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xdel probable_gq_geokeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6894cbdd-fef2-417d-a69f-3e15433c1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layne, Wagner, and Rothhaas p. 26: the name matching variables are\n",
    "# First 15 characters First Name, First 15 characters Middle Name, First 12 characters Last Name\n",
    "# Additionally, there are blocking columns for all of 1-3 initial characters of First/Last.\n",
    "# We don't have a full middle name in pseudopeople (nor would that be present in a real CUF)\n",
    "# so we have to stick to the first initial for middle.\n",
    "def add_truncated_name_cols(df):\n",
    "    df = df.assign(\n",
    "        first_name_15=df.first_name.str[:15],\n",
    "        last_name_12=df.last_name.str[:12]\n",
    "    )\n",
    "\n",
    "    if 'middle_name' in df.columns and 'middle_initial' not in df.columns:\n",
    "        df = df.assign(middle_initial=df.middle_name.str[:1])\n",
    "\n",
    "    for num_chars in [1, 2, 3]:\n",
    "        df = df.assign(**{\n",
    "            f'first_name_{num_chars}': df.first_name.str[:num_chars],\n",
    "            f'last_name_{num_chars}': df.last_name.str[:num_chars],\n",
    "        })\n",
    "\n",
    "    return df\n",
    "\n",
    "census_2030 = add_truncated_name_cols(census_2030)\n",
    "geobase_reference_file = add_truncated_name_cols(geobase_reference_file)\n",
    "name_dob_reference_file = add_truncated_name_cols(name_dob_reference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29dcc81-9056-459b-80f6-fd32125913bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layne, Wagner, and Rothhaas p. 26: phonetics are used in blocking (not matching)\n",
    "# - Soundex for Street Name\n",
    "# - NYSIIS code for First Name\n",
    "# - NYSIIS code for Last Name\n",
    "# - Reverse Soundex for First Name\n",
    "# - Reverse Soundex for Last Name\n",
    "\n",
    "def nysiis(input_string):\n",
    "    if pandas.isna(input_string):\n",
    "        return pandas.NA\n",
    "    result = jellyfish.nysiis(input_string)\n",
    "    if pandas.isna(result):\n",
    "        return pandas.NA\n",
    "\n",
    "    return result\n",
    "\n",
    "def soundex(input_string):\n",
    "    if pandas.isna(input_string):\n",
    "        return pandas.NA\n",
    "    result = jellyfish.soundex(input_string)\n",
    "    if pandas.isna(result):\n",
    "        return pandas.NA\n",
    "\n",
    "    return result\n",
    "\n",
    "def reverse_soundex(input_string):\n",
    "    if pandas.isna(input_string):\n",
    "        return pandas.NA\n",
    "\n",
    "    return soundex(input_string[::-1])\n",
    "\n",
    "def add_name_phonetics(df):\n",
    "    for col in ['first_name', 'last_name']:\n",
    "        kwargs = {}\n",
    "        if compute_engine == 'dask':\n",
    "            kwargs['meta'] = (col, 'object')\n",
    "        df = df.assign(**{f'{col}_nysiis': df[col].apply(nysiis, **kwargs)})\n",
    "        df = df.assign(**{f'{col}_reverse_soundex': df[col].apply(reverse_soundex, **kwargs)})\n",
    "\n",
    "    return df_ops.ensure_large_string_capacity(df)\n",
    "\n",
    "def add_address_phonetics(df):\n",
    "    kwargs = {}\n",
    "    if compute_engine == 'dask':\n",
    "        kwargs['meta'] = (col, 'object')\n",
    "    df = df.assign(street_name_for_blocking_soundex=df.street_name_for_blocking.apply(soundex, **kwargs))\n",
    "    return df_ops.ensure_large_string_capacity(df)\n",
    "\n",
    "census_2030 = add_name_phonetics(census_2030)\n",
    "census_2030 = add_address_phonetics(census_2030)\n",
    "\n",
    "geobase_reference_file = add_address_phonetics(geobase_reference_file)\n",
    "\n",
    "name_dob_reference_file = add_name_phonetics(name_dob_reference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe537d-10f4-4bf4-97e9-ee64fe5fcb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns used to \"cut the database\": ZIP3 and a grouping of first and last initial\n",
    "def add_zip3(df):\n",
    "    return df.assign(zip3=lambda x: x.zipcode.str[:3])\n",
    "\n",
    "def add_first_last_initial_categories(df):\n",
    "    # Page 20 of the NORC report: \"Name-cuts are defined by combinations of the first characters of the first and last names. The twenty letter groupings\n",
    "    # for the first character are: A-or-blank, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, and U-Z.\"\n",
    "    initial_cut = lambda x: x.fillna('A').str[0].replace('A', 'A-or-blank').replace(['U', 'V', 'W', 'X', 'Y', 'Z'], 'U-Z')\n",
    "    return df.assign(first_initial_cut=initial_cut(df.first_name), last_initial_cut=initial_cut(df.last_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e00367-d022-4ab2-942b-87a2b1abdfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_2030 = add_zip3(census_2030)\n",
    "census_2030 = add_first_last_initial_categories(census_2030)\n",
    "\n",
    "geobase_reference_file = add_zip3(geobase_reference_file)\n",
    "\n",
    "name_dob_reference_file = add_first_last_initial_categories(name_dob_reference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596bc8d-8f36-41bd-a6e2-a3f55c9ba66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f2911-72ad-4749-b4aa-18918335e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_2030, geobase_reference_file, name_dob_reference_file = df_ops.persist(census_2030, geobase_reference_file, name_dob_reference_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e413173-dde2-4bdb-a2cf-f0c5ef38e9a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data, ready to link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6014eaa-3548-4d23-b7b9-f8e9f0f477f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(census_2030):,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4306cf4f-d636-4d0c-9b07-7b73627982d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ops.head(census_2030, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33f5b6-36b4-48d5-9ea5-e0ae11d14dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_data_dir = f\"{output_dir}/intermediate_data\"\n",
    "if clear_intermediate:\n",
    "    utils.ensure_empty(intermediate_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c5d5f-34b6-49ec-83be-737ed625d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can save a checkpoint here for resuming without re-running the prep\n",
    "# df_ops.to_parquet(census_2030, f'{intermediate_data_dir}/census_2030_prepped.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc55f9-32dd-40e9-ba3b-0f67556ddbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(geobase_reference_file):,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361fef9-e338-4c81-934d-075dafdbd432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ops.head(geobase_reference_file, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad6959-51a8-4972-8262-81383e5a895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ops.compute(geobase_reference_file.year_of_birth.isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f87268-5fc5-43cf-b2ee-debb8b6535b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ops.to_parquet(geobase_reference_file, f'{intermediate_data_dir}/geobase_reference_file_prepped.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749e6f5-75aa-45a8-8007-7051d12019b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(name_dob_reference_file):,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf9174-67d5-47e7-a072-5b2998faf57c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ops.head(name_dob_reference_file, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4ebae-cf4c-45be-a7d7-db4062407e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ops.compute(name_dob_reference_file.year_of_birth.isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a64260-0815-456a-9166-10b7be194d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ops.to_parquet(name_dob_reference_file, f'{intermediate_data_dir}/name_dob_reference_file_prepped.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a115b7-6215-4bcf-8444-ef2223040a02",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Emulate Multi-Match with splink\n",
    "\n",
    "Wagner and Layne, p. 8:\n",
    "\n",
    "> The PVS employs its probabilistic record linkage software, Multi-Match (Wagner\n",
    "2012), as an integral part of the PVS.\n",
    "\n",
    "Wagner and Layne, p. 12:\n",
    "\n",
    "> PVS uses the same Multi-Match engine for each probabilistic search type. For each\n",
    "search module the analyst defines a parameter file, which is passed to Multi-Match. The\n",
    "parameter file includes threshold value(s) for the number of passes, blocking keys, and\n",
    "within each pass, the match variables, match comparison type, and matching weights...\n",
    ">\n",
    "> Records must first match exactly on the blocking keys before any comparisons\n",
    "between the match variables are attempted. Each match variable is given an\n",
    "m and\n",
    "u\n",
    "probability, which is translated by MultiMatch as agreement and disagreement weights.\n",
    "The sum of all match variable comparison weights for a record pair is the composite\n",
    "weight. All record pairs with a composite weight greater than or equal to the threshold\n",
    "set in the parameter file are linked, and the records from the incoming file for these\n",
    "linked cases are excluded from all remaining passes. All Numident records are always\n",
    "available for linking in every pass. Any record missing data for any of the blocking fields\n",
    "for a pass skips that pass and moves to the next pass.\n",
    "\n",
    "[splink](https://github.com/moj-analytical-services/splink) is similar to Multi-Match\n",
    "(both are based on the Fellegi-Sunter approach to record linkage).\n",
    "It also implements:\n",
    "- Exact blocking on specified keys\n",
    "- Determining overall match probability based on conditional independence of individual field comparisons\n",
    "  (it is equivalent to multiply the probability ratios, or sum the logarithmic \"weights\" as Multi-Match describes it),\n",
    "  which each have m and u probabilities\n",
    "- Setting a match threshold\n",
    "\n",
    "However, splink does not include some additional logic built into Multi-Match,\n",
    "specifically the ability to run multiple \"passes,\" removing linked records from\n",
    "subsequent passes.\n",
    "We have to implement that ourselves, calling splink again in each pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a52fa1a-85ca-4a5a-a978-08e43c5b9eb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Estimate parameters (lambda, m, u) once for all modules\n",
    "\n",
    "In Multi-Match, there is no lambda (prior probability of a link, before any comparisons are observed).\n",
    "This is because Multi-Match works entirely in weight space, and sets different weight thresholds\n",
    "in different modules/passes instead of changing the prior probability.\n",
    "\n",
    "In Multi-Match parameters are not directly estimated from the data.\n",
    "They are primarily set manually by analysts, with a different set of parameter files\n",
    "maintained for each type of input file (e.g. survey, administrative).\n",
    "\n",
    "All parameters except lambda are column-level parameters.\n",
    "We estimate the column-level parameters using the GeoBase Reference File,\n",
    "since it has all columns that are used for matching with any reference file.\n",
    "\n",
    "We estimate lambda (manually, see below) separately by reference file,\n",
    "since the number of records is different between the reference files but\n",
    "the size of the underlying population (people we are trying to match) is constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5283ef-c44f-4a19-9f58-a0d091697f1a",
   "metadata": {},
   "source": [
    "### lambda (prior probability of a match)\n",
    "\n",
    "Splink has a built-in method (estimate_probability_two_random_records_match)\n",
    "for estimating this, but it did not seem to give me reasonable estimates.\n",
    "\n",
    "We just make an informed guess here based on how much overlap we expect\n",
    "in the files, how much unintentional duplication we expect in the files (e.g. someone\n",
    "being enumerated twice), how much *intentional* duplication we have in the\n",
    "files (e.g. a record for each nickname variant), and some simplifying assumptions.\n",
    "\n",
    "Our assumptions:\n",
    "- 5% of the enumerations in the CUF are unintentional duplicates\n",
    "- 0.5% of the PIKs in the reference file are unintentional duplicates\n",
    "  (that same person is also represented with a different PIK)\n",
    "- 90% of the people in the CUF are represented in the reference files\n",
    "- Being represented in both files is independent of being unintentionally\n",
    "  or intentionally duplicated\n",
    "- Being intentionally duplicated in one file is independent of being intentionally\n",
    "  duplicated in the other (likely not true since having a name with lots of variants\n",
    "  would cause intentional duplicates in both)\n",
    "\n",
    "We do this first (before m and u probabilities) because having lambda estimated is\n",
    "useful to the EM algorithm for estimating m and u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e179c13-03b3-4100-81d7-5db54c05c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_number_true_matches(input_file, reference_file):\n",
    "    people_represented_in_input_file = (\n",
    "        len(df_ops.drop_duplicates(input_file[['record_id_raw_input_file']])) * 0.95\n",
    "    )\n",
    "    people_represented_in_reference_file = (\n",
    "        len(df_ops.drop_duplicates(reference_file[['pik']])) * 0.995\n",
    "    )\n",
    "    people_represented_in_both = people_represented_in_input_file * 0.9\n",
    "\n",
    "    # Assuming independence conditions as noted above, the number of true\n",
    "    # matches that should be found for *each* true person is the expected number\n",
    "    # of records in one file times the expected number of records in the other\n",
    "    input_file_records_per_person = people_represented_in_input_file / len(input_file)\n",
    "    reference_file_records_per_person = people_represented_in_reference_file / len(reference_file)\n",
    "    record_matches_per_person = input_file_records_per_person * reference_file_records_per_person\n",
    "\n",
    "    return people_represented_in_both * record_matches_per_person\n",
    "\n",
    "def probability_two_random_records_match(input_file, reference_file):\n",
    "    cartesian_product = len(input_file) * len(reference_file)\n",
    "    if cartesian_product == 0:\n",
    "        # Does not matter\n",
    "        return 0.5\n",
    "\n",
    "    return (\n",
    "        estimate_number_true_matches(input_file, reference_file) /\n",
    "        cartesian_product\n",
    "    )\n",
    "\n",
    "probability_two_random_records_match(census_2030, geobase_reference_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d798575-22de-4515-942c-60ecc0dbe3a0",
   "metadata": {},
   "source": [
    "### m and u probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e5cd59-59c0-4ef5-bcba-f7168f7492fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if splink_engine == 'spark':\n",
    "    # https://moj-analytical-services.github.io/splink/demos/examples/spark/deduplicate_1k_synthetic.html\n",
    "    from splink.spark.jar_location import similarity_jar_location\n",
    "\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql import types\n",
    "\n",
    "    conf = SparkConf()\n",
    "    conf.setMaster(os.getenv(\"LINKER_SPARK_MASTER_URL\", spark_master_url))\n",
    "    conf.set(\"spark.driver.memory\", \"8g\")\n",
    "    conf.set(\"spark.executor.instances\", spark_num_workers)\n",
    "    conf.set(\"spark.executor.cores\", 1)\n",
    "    conf.set(\"spark.executor.memory\", f\"{spark_memory_per_worker - spark_worker_memory_overhead_mb}m\")\n",
    "\n",
    "    conf.set(\"spark.sql.shuffle.partitions\", spark_num_workers * 5)\n",
    "    conf.set(\"spark.default.parallelism\", spark_num_workers * 5)\n",
    "\n",
    "    # Add custom similarity functions, which are bundled with Splink\n",
    "    # documented here: https://github.com/moj-analytical-services/splink_scalaudfs\n",
    "    path = similarity_jar_location()\n",
    "    conf.set(\"spark.jars\", path)\n",
    "\n",
    "    sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "    spark = SparkSession(sc)\n",
    "    checkpoints_dir = f\"{output_dir}/tmpCheckpoints\"\n",
    "    utils.ensure_empty(checkpoints_dir)\n",
    "    spark.sparkContext.setCheckpointDir(checkpoints_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42b5e3-d681-48c0-9a85-4aa695017c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = [c for c in census_2030.columns if c in geobase_reference_file.columns]\n",
    "common_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39465e9-edee-4a73-ac63-c4f060013a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_table_for_splink(df, dataset_name, columns):\n",
    "    result = (\n",
    "        df[[c for c in df.columns if c in columns]]\n",
    "    )\n",
    "    if compute_engine == 'pandas' or splink_engine == 'duckdb':\n",
    "        # Needs to all be in one process anyway\n",
    "        return df_ops.compute(result)\n",
    "    else:\n",
    "        utils.ensure_empty(f'{intermediate_data_dir}/for_splink/{dataset_name}')\n",
    "        file_path = f'{intermediate_data_dir}/for_splink/{dataset_name}/{dataset_name}_{int(time.time())}.parquet'\n",
    "        df_ops.to_parquet(result, file_path, wait=True)\n",
    "        assert splink_engine == 'spark'\n",
    "        return spark.read.parquet(file_path)\n",
    "\n",
    "if len(geobase_reference_file) > 1_000_000:\n",
    "    geobase_reference_file_for_training = geobase_reference_file.sample(frac=(1_000_000/len(geobase_reference_file)), random_state=1234)\n",
    "else:\n",
    "    geobase_reference_file_for_training = geobase_reference_file\n",
    "\n",
    "if len(census_2030) > 1_000_000:\n",
    "    census_2030_for_training = census_2030.sample(frac=(1_000_000/len(census_2030)), random_state=1234)\n",
    "else:\n",
    "    census_2030_for_training = census_2030\n",
    "\n",
    "utils.ensure_empty(f'{intermediate_data_dir}/for_splink/')\n",
    "tables_for_splink = [\n",
    "    prep_table_for_splink(geobase_reference_file_for_training, \"geobase_reference_file\", common_cols),\n",
    "    prep_table_for_splink(census_2030_for_training, \"census_2030\", common_cols)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02a447-eed8-470d-8487-1954c046bebd",
   "metadata": {},
   "source": [
    "#### Define comparison variables and levels\n",
    "\n",
    "**Variables**\n",
    "\n",
    "The most recent report, from 2023 (Brown et al., p. 29), says:\n",
    "\n",
    "> [In] GeoSearch... The typical matching variables are name, DOB, sex, and\n",
    "  various address fields...\n",
    ">\n",
    "> NameSearch... uses only name and DOB fields...\n",
    "> \n",
    "> DOBSearch... compares name, sex, and DOB...\n",
    "> \n",
    "> the Household Composition search module... attempts to find a match... based on name and DOB information\n",
    "\n",
    "So, the total list: name, DOB, sex, and \"various address fields.\"\n",
    "These address fields are listed in the PVS report (p. 34) which is admittedly from 2011:\n",
    "\n",
    "> variables used directly in matching the input file with the reference files [include]...\n",
    "> street name, street name prefix and suffix, house number, rural route and box, and ZIP code\n",
    "\n",
    "**Comparisons**\n",
    "\n",
    "Massey et al. footnote 2: \n",
    "> The PVS string comparator was developed by Winkler (1995) and measures the distance\n",
    "  between two strings on a scale from 0 to 900, where a distance score of 0 is given if\n",
    "  there is no similarity between two text strings and a score of 900 is given for an exact match.\n",
    "  The cutoff value for the string distance is set to 750 in the Name Search module.\n",
    "\n",
    "Massey and O'Hara, p. 6 footnote 7:\n",
    "> For the comparison of text strings, a prorated value\n",
    "  between the chosen agreement score and chosen disagreement score is given depending on the Jaro-Winkler\n",
    "  distance between the string in the input file and reference file.\n",
    "\n",
    "Massey et al.:\n",
    "> For numeric variables, such as year of birth, a maximum acceptable difference\n",
    "> between the variable value in the input and reference record is set. This also\n",
    "> allows for creation of an interval, or band, around year of birth to permit\n",
    "> inexact matches. Within this band, prorated agreement and disagreement weights\n",
    "> are assigned depending on the similarity of year of birth.\n",
    "\n",
    "The \"maximum acceptable distance\" implies that the m probability beyond that distance is\n",
    "zero; such pairs should never be linked.\n",
    "\n",
    "Note that this continuous \"prorated value,\" as opposed to categorizing comparison levels,\n",
    "is not possible in splink and goes outside the traditional F-S framing of m and u probabilities!\n",
    "We omit it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d9ac2-eba6-48b9-a10c-7f3515adcf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if splink_engine == 'duckdb':\n",
    "    from splink.duckdb.comparison_library import (\n",
    "        exact_match,\n",
    "        jaro_winkler_at_thresholds,\n",
    "    )\n",
    "    import splink.duckdb.comparison_level_library as cll\n",
    "elif splink_engine == 'spark':\n",
    "    from splink.spark.comparison_library import (\n",
    "        exact_match,\n",
    "        jaro_winkler_at_thresholds,\n",
    "    )\n",
    "    import splink.spark.comparison_level_library as cll\n",
    "\n",
    "def numeric_column_comparison(col_name, human_name, maximum_inexact_match_difference):\n",
    "    return {\n",
    "        \"output_column_name\": col_name,\n",
    "        \"comparison_description\": human_name,\n",
    "        \"comparison_levels\": [\n",
    "            cll.null_level(col_name),\n",
    "            cll.exact_match_level(col_name),\n",
    "            {\n",
    "                \"sql_condition\": f\"abs({col_name}_l - {col_name}_r) <= {maximum_inexact_match_difference}\",\n",
    "                \"label_for_charts\": \"Inexact match\",\n",
    "            },\n",
    "            cll.else_level(),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "numeric_columns = [\"day_of_birth\", \"month_of_birth\", \"year_of_birth\"]\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"link_only\",\n",
    "    \"comparisons\": [\n",
    "        jaro_winkler_at_thresholds(\"first_name_15\", 750 / 900),\n",
    "        jaro_winkler_at_thresholds(\"last_name_12\", 750 / 900),\n",
    "        exact_match(\"middle_initial\"),\n",
    "        numeric_column_comparison(\"day_of_birth\", \"Day of birth\", maximum_inexact_match_difference=5),\n",
    "        numeric_column_comparison(\"month_of_birth\", \"Month of birth\", maximum_inexact_match_difference=3),\n",
    "        numeric_column_comparison(\"year_of_birth\", \"Year of birth\", maximum_inexact_match_difference=5),\n",
    "        # HACK: I believe that PVS compares each part of the geokey separately (though I'm\n",
    "        # not sure what the thresholds would be).\n",
    "        # However, these columns grossly violate conditional independence, so are difficult to\n",
    "        # estimate parameters for via EM.\n",
    "        # As a workaround, we just compare the entire geokey.\n",
    "        # jaro_winkler_at_thresholds(\"street_number\", 750 / 900),\n",
    "        # jaro_winkler_at_thresholds(\"street_name\", 750 / 900),\n",
    "        # jaro_winkler_at_thresholds(\"unit_number\", 750 / 900),\n",
    "        # jaro_winkler_at_thresholds(\"zipcode\", 750 / 900),\n",
    "        jaro_winkler_at_thresholds(\"geokey\", 650 / 900),\n",
    "    ],\n",
    "    \"probability_two_random_records_match\": probability_two_random_records_match(census_2030, geobase_reference_file),\n",
    "    \"unique_id_column_name\": \"record_id\",\n",
    "    # Not sure exactly what this does, but it is necessary for some of the fancier graphs below\n",
    "    \"retain_intermediate_calculation_columns\": True,\n",
    "}\n",
    "\n",
    "if splink_engine == 'duckdb':\n",
    "    from splink.duckdb.linker import DuckDBLinker\n",
    "\n",
    "    linker = DuckDBLinker(\n",
    "        tables_for_splink,\n",
    "        settings,\n",
    "        input_table_aliases=[\"reference_file\", \"census_2030\"]\n",
    "    )\n",
    "elif splink_engine == 'spark':\n",
    "    from splink.spark.linker import SparkLinker\n",
    "    linker = SparkLinker(\n",
    "        tables_for_splink,\n",
    "        settings,\n",
    "        input_table_aliases=[\"reference_file\", \"census_2030\"],\n",
    "        spark=spark,\n",
    "    )\n",
    "\n",
    "    import warnings\n",
    "    # PySpark triggers a lot of Pandas warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248795b-29ae-4f70-b0c8-1b466112c56e",
   "metadata": {},
   "source": [
    "#### Estimate u probabilities\n",
    "\n",
    "This method seems to work well:\n",
    "- For almost all of the columns, an exact or even inexact match is relatively rare\n",
    "- Exact matches are common on ZIP code (makes sense, given our sample data is all in one ZIP)\n",
    "- Inexact matches are more common where you would expect (day and month of birth have highly constrained values)\n",
    "\n",
    "I have tested that this estimation is reproducible when run multiple times (with the same seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1d82b-02d2-45df-b450-614623c41f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "linker.estimate_u_using_random_sampling(max_pairs=1e7, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393ab32-1308-47c8-bab0-39d2b0cf6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the green bars on the left, these are the m probabilities that haven't been estimated yet\n",
    "linker.m_u_parameters_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f66f6a2-28de-41a1-8c99-e8bda2dfb3e4",
   "metadata": {},
   "source": [
    "#### Estimate m probabilities\n",
    "\n",
    "The EM algorithm implemented in Splink can be used to estimate all the parameters at once.\n",
    "However, I've found this to be extremely unstable, and the lambda and u estimates I've made\n",
    "above much better than what happens when I allow EM to mess with them.\n",
    "\n",
    "These EM session blocking rules are the result of **lots** of trial and error.\n",
    "I consistently had problems with the EM algorithm deciding that first names\n",
    "were completely wrong (\"All other comparisons\") almost as often as they were\n",
    "right.\n",
    "I can think of two reasons this might be happening:\n",
    "- The EM algorithm is just looking for two coherent clusters, and the assumption is that these\n",
    "  clusters correspond to matches and non-matches.\n",
    "  But in real life, and in our simulated data, matches and non-matches are not homogenous.\n",
    "  In particular, people who are in the same family live together and tend to share a last name,\n",
    "  and this could have been the cluster EM was finding instead of finding the exact same person.\n",
    "- Conditional independence is grossly violated by some of our columns, especially the address parts\n",
    "  and the DOB parts. This could be causing some pathological behavior.\n",
    "\n",
    "Using the EM approach below, I have at least obtained reasonable-looking m and u probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117b633-fc81-4fe6-ba1d-2d9d5a6e1b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blocking_rule_for_training = \"l.first_name_15 = r.first_name_15 and l.zipcode = r.zipcode\"\n",
    "em_session_1 = linker.estimate_parameters_using_expectation_maximisation(\n",
    "    blocking_rule_for_training,\n",
    "    # Fix lambda; u is fixed by default\n",
    "    fix_probability_two_random_records_match=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3f56f-eb87-4673-bcc4-fe2269e54db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_session_1.m_u_values_interactive_history_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eae59d-f450-46df-99b6-79b2a8dfae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_session_1.match_weights_interactive_history_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71478531-8837-48f4-8748-a65c5d8fa7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocking_rule_for_training = \"l.geokey = r.geokey\"\n",
    "em_session_2 = linker.estimate_parameters_using_expectation_maximisation(\n",
    "    blocking_rule_for_training,\n",
    "    # Fix lambda; u is fixed by default\n",
    "    fix_probability_two_random_records_match=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d48f3-7f15-40de-aa1a-78d4ed42371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_session_2.m_u_values_interactive_history_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2226bd7-8baa-4be1-bb4d-f43ac0783ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linker.match_weights_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227e7b1-d573-496a-bd6f-0eb81e75e44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linker.m_u_parameters_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc79f6-2c8b-4e65-88f9-b91d94d19f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linker.parameter_estimate_comparisons_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a026ed-761b-45ee-b44b-9259423e518e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splink_settings = linker._settings_obj.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92188bbf-e12e-42f2-8a32-b007ea05e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'{intermediate_data_dir}/splink_settings.pkl', 'wb') as f:\n",
    "    pickle.dump(splink_settings, f, protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626e377-c1b5-444e-b380-a99ae5156b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{intermediate_data_dir}/splink_settings.pkl', 'rb') as f:\n",
    "    splink_settings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9963e-73da-4b51-837d-d79deca7fecf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implement matching passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437d4c7-b848-4c19-b29d-5f48d10ce911",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# Calculate this once to save time -- mapping from record_id to record_id_raw_input_file\n",
    "# There can be multiple records (with different record_id) for the same input file record\n",
    "# (record_id_raw_input_file) because of the handling of nicknames by creating extra records.\n",
    "record_id_raw_input_file_by_record_id = census_2030[['record_id', 'record_id_raw_input_file']]\n",
    "\n",
    "all_piks = df_ops.persist(df_ops.concat([\n",
    "    name_dob_reference_file[[\"record_id\", \"pik\"]],\n",
    "    geobase_reference_file[[\"record_id\", \"pik\"]]\n",
    "], ignore_index=True))\n",
    "\n",
    "dates_of_death = df_ops.persist(\n",
    "    df_ops.read_parquet(f'{input_dir}/{data_to_use}/simulated_census_numident.parquet')\n",
    "        [['pik', 'date_of_death']]\n",
    "        .assign(date_of_death=lambda s: pd.to_datetime(s.date_of_death, format='%Y%m%d', errors='coerce'))\n",
    ")\n",
    "\n",
    "class PersonLinkageCascade:\n",
    "    def __init__(self):\n",
    "        # This dataframe will accumulate the PIKs to attach to the input file\n",
    "        self.confirmed_piks = df_ops.empty_dataframe(columns=[\"record_id_raw_input_file\", \"pik\"], dtype=str)\n",
    "        self.current_module = None\n",
    "\n",
    "    def start_module(self, *args, **kwargs):\n",
    "        assert self.current_module is None or self.current_module.confirmed\n",
    "        self.current_module = PersonLinkageModule(*args, already_confirmed_piks=self.confirmed_piks, **kwargs)\n",
    "\n",
    "    def run_matching_pass(self, *args, **kwargs):\n",
    "        self.current_module.run_matching_pass(*args, **kwargs)\n",
    "\n",
    "    def confirm_piks(self, *args, **kwargs):\n",
    "        # Make sure we are not about to confirm PIKs for any of the same files we have\n",
    "        # already PIKed\n",
    "        assert len(\n",
    "            self.current_module.provisional_links.merge(self.confirmed_piks, on='record_id_raw_input_file', how='inner')\n",
    "        ) == 0\n",
    "\n",
    "        newly_confirmed_piks = self.current_module.confirm_piks_from_provisional_links()\n",
    "        if compute_engine == 'dask':\n",
    "            # An implementation of \"checkpointing\" in Dask, so we can safely delete the source files\n",
    "            # without risking permanent loss of data we will need\n",
    "            dir_path = f'{output_dir}/intermediate_data/newly_confirmed_piks/newly_confirmed_piks_{self.current_module.name}'\n",
    "            utils.ensure_empty(dir_path)\n",
    "            file_path = f'{dir_path}/newly_confirmed_piks_{self.current_module.name}_{int(time.time())}.parquet'\n",
    "            df_ops.to_parquet(newly_confirmed_piks, file_path, wait=True)\n",
    "            newly_confirmed_piks = df_ops.read_parquet(file_path)\n",
    "\n",
    "        # We are now done with the intermediate data inside this module\n",
    "        dir_path = f'{output_dir}/intermediate_data/{self.current_module.name}/'\n",
    "        utils.remove_path(dir_path)\n",
    "\n",
    "        self.confirmed_piks = df_ops.persist(df_ops.concat([\n",
    "            self.confirmed_piks,\n",
    "            newly_confirmed_piks,\n",
    "        ], ignore_index=True))\n",
    "\n",
    "@dataclass\n",
    "class PersonLinkageModule:\n",
    "    name: str\n",
    "    reference_file: pd.DataFrame\n",
    "    reference_file_name: str\n",
    "    already_confirmed_piks: pd.DataFrame\n",
    "    cut_columns: list[str]\n",
    "    matching_columns: list[str]\n",
    "    bayes_factor_cut_columns: float = 1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.provisional_links = df_ops.empty_dataframe(columns=[\"record_id_census_2030\"], dtype=str)\n",
    "        self.confirmed = False\n",
    "        utils.ensure_empty(f'{intermediate_data_dir}/for_splink/')\n",
    "        self.reference_file_for_splink = prep_table_for_splink(self.reference_file, self.reference_file_name, self.reference_file.columns)\n",
    "        if clear_intermediate:\n",
    "            utils.ensure_empty(f'{output_dir}/intermediate_data/{self.name}/')\n",
    "        self.census_to_match = df_ops.persist(\n",
    "            census_2030\n",
    "                # Only look for matches among records that have not received a confirmed PIK\n",
    "                .merge(self.already_confirmed_piks[['record_id_raw_input_file']].assign(already_confirmed_dummy=1), on='record_id_raw_input_file', how='left')\n",
    "                .pipe(lambda df: df[df.already_confirmed_dummy.isna()])\n",
    "                .drop(columns=[\"already_confirmed_dummy\"])\n",
    "        )\n",
    "\n",
    "    def run_matching_pass(\n",
    "        self,\n",
    "        pass_name,\n",
    "        blocking_columns,\n",
    "        probability_threshold=0.99,\n",
    "        input_data_transformation=lambda x: x,\n",
    "    ):\n",
    "        assert self.confirmed == False\n",
    "\n",
    "        print(f\"Running {pass_name} of {self.name}\")\n",
    "\n",
    "        columns_needed = utils.dedupe_list([\"record_id\"] + self.cut_columns + blocking_columns + self.matching_columns)\n",
    "        print(f\"Files to link are {len(self.reference_file):,.0f} and {len(self.census_to_match):,.0f} records\")\n",
    "        if splink_engine == 'spark':\n",
    "            reference_file_for_splink = self.reference_file_for_splink.select(columns_needed)\n",
    "        else:\n",
    "            reference_file_for_splink = self.reference_file_for_splink[columns_needed]\n",
    "\n",
    "        tables_for_splink = [\n",
    "            reference_file_for_splink,\n",
    "            prep_table_for_splink(\n",
    "                self.census_to_match.pipe(input_data_transformation),\n",
    "                \"census_2030\",\n",
    "                columns_needed,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        blocking_rule_parts = [f\"l.{col} = r.{col}\" for col in self.cut_columns + blocking_columns]\n",
    "        blocking_rule = \" and \".join(blocking_rule_parts)\n",
    "\n",
    "        if splink_engine == 'spark':\n",
    "            blocking_rule = {\n",
    "                \"blocking_rule\": blocking_rule,\n",
    "                \"salting_partitions\": 10,\n",
    "            }\n",
    "\n",
    "        # We base our Splink linker for this pass on the one we trained above,\n",
    "        # but limiting it to the relevant column comparisons and updating the pass-specific\n",
    "        # settings\n",
    "        pass_splink_settings = copy.deepcopy(splink_settings)\n",
    "        pass_splink_settings[\"comparisons\"] = [\n",
    "            c for c in pass_splink_settings[\"comparisons\"] if c[\"output_column_name\"] in self.matching_columns\n",
    "        ]\n",
    "        prior_two_random_records_match = probability_two_random_records_match(\n",
    "            census_2030,\n",
    "            self.reference_file,\n",
    "        )\n",
    "        prior_odds_two_random_records_match = prior_two_random_records_match / (1 - prior_two_random_records_match)\n",
    "        # If cut columns are not used for matching, need to adjust our prior for the fact\n",
    "        # that all pairs compared will have the same values in the cut columns\n",
    "        odds_two_random_records_match = prior_odds_two_random_records_match * self.bayes_factor_cut_columns\n",
    "        pass_splink_settings[\"probability_two_random_records_match\"] = odds_two_random_records_match / (1 + odds_two_random_records_match)\n",
    "        pass_splink_settings[\"blocking_rules_to_generate_predictions\"] = [blocking_rule]\n",
    "\n",
    "        if splink_engine == 'duckdb':\n",
    "            linker = DuckDBLinker(\n",
    "                tables_for_splink,\n",
    "                pass_splink_settings,\n",
    "                # Must match order of tables_for_splink\n",
    "                input_table_aliases=[\"reference_file\", \"census_2030\"]\n",
    "            )\n",
    "        else:\n",
    "            linker = SparkLinker(\n",
    "                tables_for_splink,\n",
    "                pass_splink_settings,\n",
    "                # Must match order of tables_for_splink\n",
    "                input_table_aliases=[\"reference_file\", \"census_2030\"],\n",
    "                spark=spark,\n",
    "            )\n",
    "\n",
    "        num_comparisons = linker.count_num_comparisons_from_blocking_rule(blocking_rule)\n",
    "        print(f\"Number of pairs that will be compared: {num_comparisons:,.0f}\")\n",
    "    \n",
    "        # https://moj-analytical-services.github.io/splink/demos/06_Visualising_predictions.html#comparison-viewer-dashboard\n",
    "        # We also include some pairs below the threshold, for additional context.\n",
    "        pairs_worth_inspecting = linker.predict(threshold_match_probability=probability_threshold - 0.2)\n",
    "\n",
    "        if linker.query_sql(f'select count(*) as num from {pairs_worth_inspecting.physical_name}').num.iloc[0] > 0:\n",
    "            dashboard_file_name = f\"splink_reports/{data_to_use}/{self.name.replace(' ', '_')}__{pass_name.replace(' ', '_')}.html\"\n",
    "            pathlib.Path(dashboard_file_name).parent.mkdir(parents=True, exist_ok=True)\n",
    "            linker.comparison_viewer_dashboard(pairs_worth_inspecting, dashboard_file_name, overwrite=True)\n",
    "            from IPython.display import IFrame, display\n",
    "            display(IFrame(\n",
    "                src=f\"./{dashboard_file_name}\", width=\"100%\", height=1200\n",
    "            ))\n",
    "\n",
    "        if compute_engine == 'pandas':\n",
    "            new_provisional_links = pairs_worth_inspecting.as_pandas_dataframe()\n",
    "            new_provisional_links = new_provisional_links[new_provisional_links.match_probability >= probability_threshold]\n",
    "        elif splink_engine == 'spark':\n",
    "            # We do a tiny bit of computation in Spark directly to avoid writing more to disk than we need to\n",
    "            pairs_worth_inspecting = pairs_worth_inspecting.as_spark_dataframe()\n",
    "            new_provisional_links = pairs_worth_inspecting.filter(pairs_worth_inspecting.match_probability >= probability_threshold)\n",
    "            if pairs_worth_inspecting.rdd.getNumPartitions() < compute_engine_num_jobs * 5:\n",
    "                new_provisional_links = new_provisional_links.repartition(compute_engine_num_jobs * 5)\n",
    "            # NOTE: We do *not* utils.ensure_empty across passes here, because we need the provisional links from all passes\n",
    "            # to stick around\n",
    "            dir_path = f'{output_dir}/intermediate_data/{self.name}/{pass_name}/new_provisional_links/'\n",
    "            utils.ensure_empty(dir_path)\n",
    "            file_path = f'{dir_path}/new_provisional_links_{int(time.time())}.parquet'\n",
    "            new_provisional_links.write.mode('overwrite').parquet(file_path)\n",
    "            utils.ensure_empty(checkpoints_dir) # We no longer need these checkpoints\n",
    "            new_provisional_links = df_ops.persist(df_ops.read_parquet(file_path))\n",
    "        else:\n",
    "            dir_path = f'{output_dir}/intermediate_data/{self.name}/{pass_name}/pairs_worth_inspecting/'\n",
    "            utils.ensure_empty(dir_path)\n",
    "            file_path = f'{dir_path}/pairs_worth_inspecting_{int(time.time())}.parquet'\n",
    "            pairs_worth_inspecting.to_parquet(file_path, overwrite=True)\n",
    "            pairs_worth_inspecting = df_ops.persist(df_ops.read_parquet(file_path))\n",
    "            new_provisional_links = pairs_worth_inspecting[pairs_worth_inspecting.match_probability >= probability_threshold]\n",
    "            del pairs_worth_inspecting\n",
    "\n",
    "        if splink_engine == 'spark':\n",
    "            utils.ensure_empty(checkpoints_dir) # We are done with this Spark linker\n",
    "\n",
    "        if len(new_provisional_links) > 0:\n",
    "            new_provisional_links = df_ops.persist(\n",
    "                label_pairs_with_dataset(new_provisional_links)\n",
    "                    .merge(record_id_raw_input_file_by_record_id, left_on=\"record_id_census_2030\", right_on=\"record_id\", how=\"left\")\n",
    "                    .drop(columns=[\"record_id\"])\n",
    "            )\n",
    "            self.provisional_links = df_ops.persist(df_ops.concat([\n",
    "                self.provisional_links,\n",
    "                new_provisional_links.assign(module_name=self.name, pass_name=pass_name).pipe(df_ops.ensure_large_string_capacity)\n",
    "            ], ignore_index=True))\n",
    "\n",
    "            self.census_to_match = df_ops.persist(\n",
    "                self.census_to_match\n",
    "                    # Only look for matches among records that have not received a provisional link\n",
    "                    # NOTE: \"records\" here does not mean input file records -- a nickname record having\n",
    "                    # a provisional link does not prevent a canonical name record from the same input record\n",
    "                    # from continuing to match\n",
    "                    .merge(df_ops.drop_duplicates(new_provisional_links[['record_id_census_2030']]), left_on='record_id', right_on='record_id_census_2030', how='left')\n",
    "                    .pipe(lambda df: df[df.record_id_census_2030.isna()])\n",
    "                    .drop(columns=[\"record_id_census_2030\"])\n",
    "            )\n",
    "\n",
    "        print(f'Found {len(new_provisional_links):,.0f} matches; {len(self.census_to_match) / len(census_2030):.2%} still eligible to match')\n",
    "\n",
    "    def confirm_piks_from_provisional_links(self):\n",
    "        assert not self.confirmed\n",
    "\n",
    "        provisional_links = (\n",
    "            self.provisional_links\n",
    "                .merge(all_piks, left_on=\"record_id_reference_file\", right_on=\"record_id\", how=\"left\")\n",
    "                .drop(columns=\"record_id\")\n",
    "        )\n",
    "\n",
    "        # \"After the initial set of links is created in GeoSearch, a post-search program is run to determine\n",
    "        # which of the links are retained. A series of checks are performed: First the date of death\n",
    "        # information from the Numident is checked and links to deceased persons are dropped. Next a\n",
    "        # check is made for more than one SSN assigned to a source record. If more than one SSN is\n",
    "        # assigned, the best link is selected based on match weights. If no best SSN is determined, all SSNs\n",
    "        # assigned in the GeoSearch module are dropped and the input record cascades to the next\n",
    "        # module. A similar post-search program is run at the end of all search modules.\"\n",
    "        # - Layne et al. p. 5\n",
    "\n",
    "        # Drop links to deceased people\n",
    "        # NOTE: On p. 38 of Brown et al. (2023) it discusses at length the number of PVS matches to deceased\n",
    "        # people, which should not be possible based on this process.\n",
    "        # Even though this is more recent, I can't think of a reason why this check would have\n",
    "        # been *removed* from PVS -- can we chalk this up to something experimental they were doing for\n",
    "        # the AR Census in the 2023 report?\n",
    "        provisional_links = df_ops.persist(\n",
    "            provisional_links.merge(dates_of_death, on=\"pik\", how=\"left\")\n",
    "        )\n",
    "        # Census day 2030\n",
    "        deceased_links = df_ops.persist(provisional_links.date_of_death.notnull() & (provisional_links.date_of_death <= pandas.to_datetime(\"2030-04-01\")))\n",
    "        print(f'{df_ops.compute(deceased_links.sum()):,.0f} input records linked to deceased people, dropping links')\n",
    "        provisional_links = df_ops.persist(provisional_links[~deceased_links])\n",
    "\n",
    "        # Check for multiple linkage to a single input file record\n",
    "        max_probability = df_ops.groupby_agg_small_groups(provisional_links, by=\"record_id_raw_input_file\", agg_func=lambda x: x.match_probability.max()).reset_index()\n",
    "        piks_per_input_file = (\n",
    "            provisional_links\n",
    "                .merge(max_probability, on=[\"record_id_raw_input_file\", \"match_probability\"])\n",
    "                .pipe(df_ops.groupby_agg_small_groups, by=\"record_id_raw_input_file\", agg_func=lambda x: x.pik.nunique())\n",
    "                .rename('num_unique_piks')\n",
    "                .reset_index()\n",
    "        )\n",
    "\n",
    "        multiple_piks = df_ops.persist(piks_per_input_file[piks_per_input_file.num_unique_piks > 1])\n",
    "        print(f'{len(multiple_piks):,.0f} input records linked to multiple PIKs, dropping links')\n",
    "        provisional_links = df_ops.persist(\n",
    "            provisional_links\n",
    "                .merge(multiple_piks, on=\"record_id_raw_input_file\", how=\"left\")\n",
    "                .pipe(lambda df: df[df.num_unique_piks.isnull() | (df.num_unique_piks <= 1)])\n",
    "                .pipe(df_ops.drop_duplicates, subset=\"record_id_raw_input_file\", sort_col=\"match_probability\", keep=\"last\")\n",
    "        )\n",
    "\n",
    "        assert df_ops.compute((df_ops.groupby_agg_small_groups(provisional_links, by=\"record_id_raw_input_file\", agg_func=lambda x: x.pik.nunique()) == 1).all())\n",
    "\n",
    "        self.confirmed = True\n",
    "        self.provisional_links = None\n",
    "        \n",
    "        return (\n",
    "            provisional_links[[\n",
    "                \"record_id_raw_input_file\",\n",
    "                \"record_id_census_2030\",\n",
    "                \"record_id_reference_file\",\n",
    "                \"pik\",\n",
    "                \"module_name\",\n",
    "                \"pass_name\",\n",
    "                \"match_probability\",\n",
    "            ]]\n",
    "        )\n",
    "\n",
    "def label_pairs_with_dataset(pairs):\n",
    "    pairs = df_ops.persist(pairs)\n",
    "    if len(pairs) == 0:\n",
    "        return pairs\n",
    "\n",
    "    # Name the columns according to the datasets, not \"r\" (right) and \"l\" (left)\n",
    "    for suffix in [\"r\", \"l\"]:\n",
    "        # NOTE: In practice, splink always has \"r\" correspond to one dataset and \"l\" correspond to the other.\n",
    "        # It is unclear to me if this is a documented guarantee, so I double check.\n",
    "        # NOTE: Do not use drop_duplicates here, since the groups should be enormous (the entire dataframe)\n",
    "        assert df_ops.compute(pairs[f'source_dataset_{suffix}'].nunique()) == 1\n",
    "        source_dataset_name = df_ops.head(pairs, n=1)[f'source_dataset_{suffix}'].iloc[0]\n",
    "        pairs = pairs.rename(columns={c: re.sub(f'_{suffix}$', f'_{source_dataset_name}', c) for c in pairs.columns if c.endswith(f'_{suffix}')})\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b7236a-ef52-4151-acab-ba1f0a186042",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade = PersonLinkageCascade()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc1b23-3bbf-4571-a89d-9bf8db4c0dbe",
   "metadata": {},
   "source": [
    "# Verification Module\n",
    "\n",
    "Wagner and Layne, p.14:\n",
    "\n",
    "> If the input file has a SSN data field, it first goes through the verification process.\n",
    "\n",
    "This module of PVS only runs on input records with an SSN.\n",
    "No records in the CUF have an SSN (SSN is not collected on the decennial)\n",
    "so this module is skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0203571-b01f-477b-879d-c1c7587020d7",
   "metadata": {},
   "source": [
    "# GeoSearch\n",
    "\n",
    "Brown et al. p. 29:\n",
    "\n",
    "> the GeoSearch module blocks records at\n",
    "different levels of geography, starting with the housing unit level, then broadening geography\n",
    "up to the three-digit ZIP Code level. The typical matching variables are name, DOB, sex, and\n",
    "various address fields.\n",
    "\n",
    "Wagner and Layne p. 14:\n",
    "\n",
    "> The typical GeoSearch blocking strategy starts with blocking records at the\n",
    "household level, then broadens the geography for each successive pass and ends at\n",
    "blocking by the first three digits of the zip code. The typical match variables are first,\n",
    "middle, and last names; generational suffix; date of birth; gender and various address\n",
    "fields.\n",
    "> \n",
    "> The data for the GeoSearch module are split into 1,000 cuts based on the first three\n",
    "digits of the zip code (zip3) for record. The GeoSearch program works on one zip3 cut\n",
    "at a time, with shell scripts submitting multiple streams of cuts to the system. This\n",
    "allows for parallel processing and restart capability.\n",
    "\n",
    "Layne, Wagner, and Rothhaas list an exact series of passes with blocking and matching variables\n",
    "for each (Appendix A on p. 25).\n",
    "This was the series of passes they used in running PVS on the Medicare Enrollment\n",
    "Database, the Indian Health Service Patient Registration File, and on two commercial\n",
    "data sources.\n",
    "All of these data sources are rather different than a CUF, and this paper is from 2014,\n",
    "but it is the only place I know of where an entire set of passes is described,\n",
    "so I have used the exact same passes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560948a-c9ca-4b96-9fa9-4c4b942689e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.start_module(\n",
    "    name=\"geosearch\",\n",
    "    reference_file=geobase_reference_file,\n",
    "    reference_file_name=\"geobase_reference_file\",\n",
    "    cut_columns=[\"zip3\"],\n",
    "    matching_columns=[\n",
    "        \"first_name_15\",\n",
    "        \"last_name_12\",\n",
    "        \"middle_initial\",\n",
    "        \"day_of_birth\",\n",
    "        \"month_of_birth\",\n",
    "        \"year_of_birth\",\n",
    "        \"geokey\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0398ac85-1ff3-4103-979c-ff00936ced02",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pass 1: Block on geokey (entire address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13afeba-9198-4546-9092-6ee563cc51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"geokey\",\n",
    "    blocking_columns=[\"geokey_for_blocking\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef782163-425e-4745-a8a4-f352032ee9d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pass 2: Block on geokey (entire address), switching first and last names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83383f23-807c-49bb-88b7-7ef69d9a6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_first_and_last_names(df):\n",
    "    return (\n",
    "        df.rename(columns={\"first_name\": \"last_name\", \"last_name\": \"first_name\"})\n",
    "            # Re-calculate the truncated versions of first and last.\n",
    "            # NOTE: It is not necessary to re-calculate the phonetic versions, because\n",
    "            # those are never used in any pass that has a name switch.\n",
    "            .pipe(add_truncated_name_cols)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a30656-2fad-43f2-8712-6e52fb8c94ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We don't actually have any swapping of first and last names in pseudopeople\n",
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"geokey name switch\",\n",
    "    blocking_columns=[\"geokey_for_blocking\"],\n",
    "    input_data_transformation=switch_first_and_last_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1abcb7-8577-428a-a531-5fac5f3ea6dc",
   "metadata": {},
   "source": [
    "## Pass 3: Block on house number and street name Soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c841a9-09af-4708-a094-2ddc19526648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"house number and street name Soundex\",\n",
    "    blocking_columns=[\"street_number_for_blocking\", \"street_name_for_blocking_soundex\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b68b0c-f33b-42bb-a80f-22afae888216",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pass 4: Block on house number and street name Soundex, switching first and last names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7665776-b592-4762-b4ca-f399ce9a95ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"house number and street name Soundex name switch\",\n",
    "    blocking_columns=[\"street_number_for_blocking\", \"street_name_for_blocking_soundex\"],\n",
    "    input_data_transformation=switch_first_and_last_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4753574-d776-4b83-9ca8-762a2d665ddc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pass 5: Block on some name and DOB information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1924438b-d5ec-4bc7-a083-46d0165c9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"some name and DOB information\",\n",
    "    blocking_columns=[\"first_name_2\", \"last_name_2\", \"year_of_birth\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f507b1e-61e6-448f-bc2e-afccc3020356",
   "metadata": {},
   "source": [
    "## Post-process and confirm PIKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30899aeb-e35f-4246-979c-af9f8235c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.confirm_piks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea4fa2-588b-4555-a78e-8747da8db6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ops.compute(person_linkage_cascade.confirmed_piks.groupby([\"module_name\", \"pass_name\"]).size()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ae657-5248-4a78-b836-3a04e8732140",
   "metadata": {},
   "source": [
    "# ZIP3 Adjacency Search\n",
    "\n",
    "Layne, Wagner, and Rothhaas (p. 6) refers to ZIP3 adjacency as a module and even provides\n",
    "a list of two passes within it in Appendix A.\n",
    "\n",
    "Alexander et al. (p. 6) refers to it the same way, as a module.\n",
    "\n",
    "However, Wagner and Layne (p. 14) says:\n",
    "\n",
    "> The GeoSearch module also incorporates the adjacency of neighboring areas with\n",
    "different zip3 values\n",
    "\n",
    "which implies this logic is a pass or passes *within* GeoSearch (and due to how cascading works,\n",
    "this is not just an academic distinction).\n",
    "\n",
    "There is no mention at all of ZIP3 adjacency in the Brown et al. paper from 2023.\n",
    "\n",
    "**TODO**: Determine whether this still exists, how it works if so, and implement it.\n",
    "Doesn't really make sense with the sample data since there is only one ZIP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19c7f2-e129-4c96-b22e-a2983c40a15c",
   "metadata": {},
   "source": [
    "# Movers\n",
    "\n",
    "The Movers module is not mentioned in Brown et al, nor in Wagner and Layne.\n",
    "\n",
    "In Layne, Wagner, and Rothhaas (p. 7), published in 2014, it is described as \"Prototyped only – not implemented in PVS\":\n",
    "\n",
    "> The Movers module is appropriate for input files that combine individuals together into\n",
    "> households.\n",
    "> The module seeks multiple members of an input household in one address that\n",
    "> may have moved together to another address.\n",
    "> To be eligible for this module the household size must be greater than one.\n",
    "> This module is being tested and was not used in the analysis for this paper.\n",
    "\n",
    "Alexander et al. (p. 6) a year later makes no note of it being experimental:\n",
    "\n",
    "> To be eligible for the Movers Module, no member of the household can have a\n",
    "> PIK and the household must consist of more than one member.\n",
    "> This module considers persons living at the same address as a unit and\n",
    "> searches for matching units living together in the reference file (without regard for address).\n",
    "\n",
    "Essentially the same text is included in Massey et al., published in 2018.\n",
    "\n",
    "Though it doesn't provide much information, [this document](https://gunnisonconsulting.com/docs/pdf/Record%20Linkage%20Slicksheet_FINAL.pdf)\n",
    "([archive](https://web.archive.org/web/20230705010237/https://gunnisonconsulting.com/docs/pdf/Record%20Linkage%20Slicksheet_FINAL.pdf))\n",
    "from the Gunnison consulting group states that they worked with Census and\n",
    "\"pioneered the transition from a record-by-record matching approach to an\n",
    "approach that better uses groups of records, such as full households, to make more\n",
    "and better links.\"\n",
    "Specifically, they claim that 16% of previously unlinked records were linked using\n",
    "the household-level approach.\n",
    "\n",
    "**TODO**: Haven't figured out if this is still in use, nor how it works.\n",
    "This module seems of particular interest, because it sounds like it is quite different\n",
    "from other modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7412843-d6c2-4064-ab38-ab9335b9d71e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NameSearch\n",
    "\n",
    "Brown et al., p. 29:\n",
    "\n",
    "> ... the NameSearch module, which uses only name and DOB fields, comparing all combinations of\n",
    "alternate names and dates of birth.\n",
    "\n",
    "Wagner and Layne, p. 15:\n",
    "\n",
    "> The NameSearch module searches the reference files for records failing the\n",
    "Verification and GeoSearch Modules. Only name and date of birth data are used in this\n",
    "search process. NameSearch consists of multiple passes against the Numident Name\n",
    "Reference file, which contains all possible combinations of alternate names and\n",
    "alternate dates of birth for each SSN in the Census Numident file, and includes data for\n",
    "ITINs.\n",
    "> \n",
    "> The typical NameSearch blocking strategy starts with a strict first pass, blocking\n",
    "records by exact date of birth and parts of names. Successive passes block on parts of\n",
    "the name and date of birth fields to allow for some name and date of birth variation.\n",
    "The typical match variables are first, middle and last names, generational suffix, date of\n",
    "birth, and gender.\n",
    "\n",
    "As in GeoSearch, the only full listing I could find of passes with blocking and matching variables\n",
    "was in Layne, Wagner, and Rothhaas.\n",
    "This is probably somewhat out of date and is not what would be used on a CUF, but I have\n",
    "copied it exactly here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf41a6a-8c72-4c9a-bb8c-410d7877414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.start_module(\n",
    "    name=\"namesearch\",\n",
    "    reference_file=name_dob_reference_file,\n",
    "    reference_file_name=\"name_dob_reference_file\",\n",
    "    cut_columns=[\"first_initial_cut\", \"last_initial_cut\"],\n",
    "    matching_columns=[\n",
    "        \"first_name_15\",\n",
    "        \"last_name_12\",\n",
    "        \"middle_initial\",\n",
    "        \"day_of_birth\",\n",
    "        \"month_of_birth\",\n",
    "        \"year_of_birth\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43e928-be1c-4f2a-a747-e624c19d40d6",
   "metadata": {},
   "source": [
    "## Pass 1: Block on DOB and NYSIIS of name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5baab-1b94-4359-9398-c829ebb2a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"DOB and NYSIIS of name\",\n",
    "    blocking_columns=[\"day_of_birth\", \"month_of_birth\", \"year_of_birth\", \"first_name_nysiis\", \"last_name_nysiis\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbec09-ed90-4d7d-9fe3-4a934b6d8bce",
   "metadata": {},
   "source": [
    "## Pass 2: Block on DOB and initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011e74c-b87e-436c-9c75-5fe23fafdd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"DOB and initials\",\n",
    "    blocking_columns=[\"day_of_birth\", \"month_of_birth\", \"year_of_birth\", \"first_name_1\", \"last_name_1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e5dad-fbb8-4e7f-9b31-8ef079623b71",
   "metadata": {},
   "source": [
    "## Pass 3: Block on year of birth and first two characters of name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc57ab-bf73-4524-a82a-4242a7e37be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"year of birth and first two characters of name\",\n",
    "    blocking_columns=[\"year_of_birth\", \"first_name_2\", \"last_name_2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980962d2-3da2-4ca1-82b6-6452e6fce0f8",
   "metadata": {},
   "source": [
    "## Pass 4: Block on birthday and first two characters of name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea04745-1b41-4e13-8975-bb2e015c3fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"birthday and first two characters of name\",\n",
    "    blocking_columns=[\"day_of_birth\", \"month_of_birth\", \"first_name_2\", \"last_name_2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e0d7c-54d8-4ce5-9723-0650a2a58924",
   "metadata": {},
   "source": [
    "## Post-process and confirm PIKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7588d-85c3-4087-b6db-17dc31859cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.confirm_piks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892c48c-c5d5-44c6-9439-2f59ebc053b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ops.compute(person_linkage_cascade.confirmed_piks.groupby([\"module_name\", \"pass_name\"]).size()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf67411-44e7-4ae8-926c-9b1f56d4522f",
   "metadata": {},
   "source": [
    "# DOBSearch\n",
    "\n",
    "Brown et al., p. 29:\n",
    "\n",
    "> ... the DOBSearch module, which blocks on\n",
    "month and day of birth, then compares name, sex, and DOB.\n",
    "\n",
    "Wagner and Layne, p. 15:\n",
    "\n",
    "> The DOBSearch module searches the reference files for the records that fail the\n",
    "> NameSearch, using name and date of birth data. The module matches against a re-split\n",
    "> version of the Numident Name Reference file, splitting the data based on month and day\n",
    "> of birth.\n",
    "> \n",
    "> There are typically four blocking passes in the DOBSearch module. The first pass\n",
    "> blocks records by first name in the incoming file to last name in the DOB Reference file\n",
    "> and last name in incoming file to first name in the DOB Reference file. This strategy\n",
    "> accounts for switching of first and last name in the incoming file.\n",
    "\n",
    "Again, I have used the exact passes listed for this module in Layne, Wagner, and Rothhaas.\n",
    "This is somewhat corroborated to be \"typical\" given that it is consistent with Wagner and Layne\n",
    "above, but both papers are old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbc963-25e2-4f17-9721-f855589dd28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.start_module(\n",
    "    name=\"dobsearch\",\n",
    "    reference_file=name_dob_reference_file,\n",
    "    reference_file_name=\"name_dob_reference_file\",\n",
    "    cut_columns=[\"day_of_birth\", \"month_of_birth\"],\n",
    "    matching_columns=[\n",
    "        \"first_name_15\",\n",
    "        \"last_name_12\",\n",
    "        \"middle_initial\",\n",
    "        \"day_of_birth\",\n",
    "        \"month_of_birth\",\n",
    "        \"year_of_birth\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6013e-ea45-463d-8095-ef3e4c8abf20",
   "metadata": {},
   "source": [
    "## Pass 1: Block on initials, switching first and last names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa73d4-ee01-4a35-94f9-ab0eb6b09132",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"initials name switch\",\n",
    "    blocking_columns=[\"first_name_1\", \"last_name_1\"],\n",
    "    input_data_transformation=switch_first_and_last_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa68a9b-6f38-4f7c-8246-65a3bd780f54",
   "metadata": {},
   "source": [
    "## Pass 2: Block on first three characters of name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d3d033-96ae-43bf-8f90-c73036c39d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"first three characters of name\",\n",
    "    blocking_columns=[\"first_name_3\", \"last_name_3\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d06a88-895a-469b-a830-57c46c58ae45",
   "metadata": {},
   "source": [
    "## Pass 3: Block on reverse Soundex of name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e24ef-2ae2-4ca7-8d92-05630a1d61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"reverse Soundex of name\",\n",
    "    blocking_columns=[\"first_name_reverse_soundex\", \"last_name_reverse_soundex\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4da3d-c9c7-4c50-b409-0bf76920828f",
   "metadata": {},
   "source": [
    "## Pass 4: Block on first two characters of first name and year of birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e39510-cabb-46a6-99f8-e1969a8eb364",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"first two characters of first name and year of birth\",\n",
    "    blocking_columns=[\"first_name_2\", \"year_of_birth\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e3db4-998e-40cd-9e33-ef6cf6e99b9b",
   "metadata": {},
   "source": [
    "## Post-process and confirm PIKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9581f811-9d21-449d-8682-97eb0ef0e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.confirm_piks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853bb87d-ac28-41b1-a661-63d38169615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ops.compute(person_linkage_cascade.confirmed_piks.groupby([\"module_name\", \"pass_name\"]).size()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab65b7-a7ff-46e9-bc54-397bdeb8f6e5",
   "metadata": {},
   "source": [
    "# HHCompSearch\n",
    "\n",
    "Brown et al., p. 29:\n",
    "\n",
    "> ... the Household Composition search module, which requires at least one\n",
    "> person in the household of the unmatched person to have received a PIK.\n",
    "> The full set of unmatched records with historical name, DOB, sex, and address data from\n",
    "> households whose members with PIKs were observed in the past is created. The module\n",
    "> attempts to find a match to this universe based on name and DOB information.\n",
    "\n",
    "Wagner and Layne, p. 16:\n",
    "\n",
    "> For persons with a PIK in the eligible household, all of the geokeys from the PVS\n",
    "> GeoBase are extracted for each of these PIKs. The geokeys are unduplicated and all\n",
    "> persons are selected from the PVS GeoBase with these geokeys. Next, the program\n",
    "> removes all household members with a PIK, leaving the unPIKed persons in the\n",
    "> household. This becomes the reference file to search against. There are typically two\n",
    "> passes in this module. Records are blocked by MAFID, name, date of birth, and gender.\n",
    "\n",
    "I don't know of any list of passes in this module.\n",
    "Blocking on \"MAFID, name, date of birth, and gender\" seems like a lot for only\n",
    "two passes, especially given that we are only matching people within each eligible\n",
    "household.\n",
    "I'm actually a bit surprised there is any blocking at all, given how restrictive this\n",
    "module inherently is.\n",
    "I've split the difference here by creating two passes that are very permissive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd285ed-2445-42d2-8942-3bb9d241dd16",
   "metadata": {},
   "source": [
    "## Create the reference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793c25f-96a7-406f-b53b-ac15cc804b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: As of now in pseudopeople, our only indicator in the Census data of household\n",
    "# is noiseless (ground truth).\n",
    "# Instead, we use geokey to approximate the household clustering information that would be\n",
    "# present in the real dataset.\n",
    "# We should switch to using a (presumably low-noise) household indicator when we have that.\n",
    "# NOTE: We use geokey_for_blocking instead of geokey to intentionally exclude the very\n",
    "# large households currently created by GQ in pseudopeople.\n",
    "pseudo_household_id = (\n",
    "    df_ops.drop_duplicates(census_2030[census_2030.geokey_for_blocking.notnull()][[\"geokey_for_blocking\"]])\n",
    "        .pipe(df_ops.add_unique_id_col, col_name=\"pseudo_household_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb318684-15c2-4e12-857a-39cd92df6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_2030 = df_ops.persist(census_2030.merge(pseudo_household_id, on=\"geokey_for_blocking\", how=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5088f0-7139-44a0-b701-6b50b103dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "piks_with_household = df_ops.persist(\n",
    "    census_2030[[\"pseudo_household_id\", \"record_id_raw_input_file\"]]\n",
    "        .merge(person_linkage_cascade.confirmed_piks, on=\"record_id_raw_input_file\", how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d11a0-6ecd-4a41-9b94-606bbc45831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "piked_by_household = df_ops.groupby_agg_small_groups(\n",
    "    piks_with_household,\n",
    "    by=\"pseudo_household_id\",\n",
    "    agg_func=lambda x: x.pik.agg(['count', 'size'])\n",
    ").reset_index().rename(columns={'count': 'piked'}).assign(unpiked=lambda x: x['size'] - x['piked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02350ff8-890c-4d4f-a7c4-dd666e0af152",
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible_households = piked_by_household[(piked_by_household.piked > 0) & (piked_by_household.unpiked > 0)][['pseudo_household_id']]\n",
    "eligible_households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a671a0b0-8713-4e29-8db4-bd3a40162c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "piks_by_eligible_household = df_ops.drop_duplicates(\n",
    "    eligible_households\n",
    "        .merge(piks_with_household[[\"pseudo_household_id\", \"pik\"]].dropna(subset=\"pik\"), on=\"pseudo_household_id\", how=\"inner\")\n",
    ")\n",
    "piks_by_eligible_household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386f872-93a4-48b1-88bd-82db34a6df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "geokeys_by_eligible_household = (\n",
    "    piks_by_eligible_household\n",
    "        .merge(df_ops.drop_duplicates(geobase_reference_file[[\"pik\", \"geokey_for_blocking\"]].dropna(subset=\"geokey_for_blocking\")), on=\"pik\")\n",
    "        .drop(columns=[\"pik\"])\n",
    "        .pipe(df_ops.drop_duplicates)\n",
    ")\n",
    "geokeys_by_eligible_household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034b8a2-a959-41a6-bce5-b31b3f7bad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently, we exclude from the reference file all *reference file* records with a PIK that\n",
    "# has already been assigned to an input file row.\n",
    "# Doing this goes against the normal assumption, which is that reference file records can match\n",
    "# to multiple input file records.\n",
    "# This is really surprising to me, but it seems clear from \"the program\n",
    "# removes all household members with a PIK, leaving the unPIKed persons in the\n",
    "# household. This becomes the reference file to search against.\" (Wagner and Layne, p. 16)\n",
    "eligible_geobase_reference_file_records = (\n",
    "    geobase_reference_file[geobase_reference_file.geokey_for_blocking.notnull()]\n",
    "        .merge(df_ops.drop_duplicates(person_linkage_cascade.confirmed_piks[['pik']]).assign(confirmed_pik_dummy=1), on='pik', how='left')\n",
    "        .pipe(lambda df: df[df.confirmed_pik_dummy.isnull()])\n",
    "        .drop(columns=['confirmed_pik_dummy'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5a0f4-6220-4d29-a604-52b930c3198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hhcomp_reference_file = df_ops.persist(geokeys_by_eligible_household.merge(\n",
    "    eligible_geobase_reference_file_records,\n",
    "    on=\"geokey_for_blocking\",\n",
    "))\n",
    "hhcomp_reference_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26ecbb-d8ff-4ac5-bffa-6d21e8dc3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hhcomp_reference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a1a66-1c18-48ad-a76f-11150f71a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.start_module(\n",
    "    name=\"hhcompsearch\",\n",
    "    reference_file=hhcomp_reference_file,\n",
    "    reference_file_name=\"hhcomp_reference_file\",\n",
    "    cut_columns=[\"pseudo_household_id\"],\n",
    "    # HACK: How else do we deal with the fact that pseudo_household_id\n",
    "    # isn't used for matching?\n",
    "    bayes_factor_cut_columns=1_000,\n",
    "    matching_columns=[\n",
    "        \"first_name_15\",\n",
    "        \"last_name_12\",\n",
    "        \"middle_initial\",\n",
    "        \"day_of_birth\",\n",
    "        \"month_of_birth\",\n",
    "        \"year_of_birth\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86727cf-b024-4caf-a610-6f7950faec2d",
   "metadata": {},
   "source": [
    "## Pass 1: Block on initials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fbf88d-044e-4764-a049-ad6c4520bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"initials\",\n",
    "    blocking_columns=[\"first_name_1\", \"last_name_1\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08637b-760c-4633-b0c4-23a6d4c01dd1",
   "metadata": {},
   "source": [
    "## Pass 2: Block on year of birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbc8e1-230c-4b59-95b4-e8ba97d566a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.run_matching_pass(\n",
    "    pass_name=\"year of birth\",\n",
    "    blocking_columns=[\"year_of_birth\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd99a1-3174-435f-bd22-49ffa06ec1d6",
   "metadata": {},
   "source": [
    "## Post-process and confirm PIKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61a681-a87d-4a4a-a89e-146b044fd22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_linkage_cascade.confirm_piks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f147e2d-7510-4aad-bc30-1bbda2b4091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ops.compute(person_linkage_cascade.confirmed_piks.groupby([\"module_name\", \"pass_name\"]).size()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ba651-913d-4eb4-9d3a-e391b9efc8d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Resulting PIKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a239d1-e1d8-435d-9af5-7c71ba3d64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pik_values = df_ops.drop_duplicates(\n",
    "    person_linkage_cascade.confirmed_piks\n",
    "        .rename(columns={\"record_id_raw_input_file\": \"record_id\"})\n",
    "        [[\"record_id\", \"pik\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9271823-d2b3-4f41-9d77-9ad5bb117397",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_2030_piked = census_2030_raw_input.copy()\n",
    "kwargs = {}\n",
    "if compute_engine != 'dask':\n",
    "    kwargs['validate'] = '1:1'\n",
    "census_2030_piked = df_ops.persist(census_2030_piked.merge(\n",
    "    pik_values,\n",
    "    how=\"left\",\n",
    "    on=\"record_id\",\n",
    "    **kwargs,\n",
    "))\n",
    "census_2030_piked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6566f0-e77b-4b21-968c-05a7ef08eff6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "piked_proportion = census_2030_piked.pik.notnull().mean()\n",
    "# Compare with 90.28% of input records PIKed in the 2010 CUF,\n",
    "# as reported in Wagner and Layne, Table 2, p. 18\n",
    "print(f'{df_ops.compute(piked_proportion):.2%} of the input records were PIKed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc185e2-1d4e-4d8c-98a0-3c0c3d1641cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ops.to_parquet(census_2030_piked, f'{output_dir}/census_2030_piked.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822eb452-ca69-48fc-b1d6-c3425fd5f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ops.to_parquet(person_linkage_cascade.confirmed_piks, f'{output_dir}/confirmed_piks.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee7cdb-bac3-4245-b1e1-0cdcb6652a27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: Can't use the magic ! date here for reasons I don't really understand having\n",
    "# to do with the number of open file handles. This works fine.\n",
    "print(datetime.datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "person_linkage_case_study_20240423",
   "language": "python",
   "name": "person_linkage_case_study_20240423"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
